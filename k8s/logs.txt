
==> Audit <==
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  COMMAND   ‚îÇ        ARGS         ‚îÇ PROFILE  ‚îÇ  USER  ‚îÇ VERSION ‚îÇ     START TIME      ‚îÇ      END TIME       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ start      ‚îÇ                     ‚îÇ minikube ‚îÇ aakash ‚îÇ v1.37.0 ‚îÇ 30 Nov 25 10:37 CST ‚îÇ 30 Nov 25 10:37 CST ‚îÇ
‚îÇ docker-env ‚îÇ minikube docker-env ‚îÇ minikube ‚îÇ aakash ‚îÇ v1.37.0 ‚îÇ 30 Nov 25 10:38 CST ‚îÇ 30 Nov 25 10:38 CST ‚îÇ
‚îÇ service    ‚îÇ nginx               ‚îÇ minikube ‚îÇ aakash ‚îÇ v1.37.0 ‚îÇ 30 Nov 25 10:38 CST ‚îÇ                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò


==> Last Start <==
Log file created at: 2025/11/30 10:37:19
Running on machine: MSI
Binary: Built with gc go1.24.6 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1130 10:37:19.027912    5920 out.go:360] Setting OutFile to fd 1 ...
I1130 10:37:19.028014    5920 out.go:413] isatty.IsTerminal(1) = true
I1130 10:37:19.028016    5920 out.go:374] Setting ErrFile to fd 2...
I1130 10:37:19.028018    5920 out.go:413] isatty.IsTerminal(2) = true
I1130 10:37:19.028140    5920 root.go:338] Updating PATH: /home/aakash/.minikube/bin
W1130 10:37:19.028216    5920 root.go:314] Error reading config file at /home/aakash/.minikube/config/config.json: open /home/aakash/.minikube/config/config.json: no such file or directory
I1130 10:37:19.029205    5920 out.go:368] Setting JSON to false
I1130 10:37:19.032038    5920 start.go:130] hostinfo: {"hostname":"MSI","uptime":6056,"bootTime":1764514584,"procs":46,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"24.04","kernelVersion":"6.6.87.2-microsoft-standard-WSL2","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"guest","hostId":"cc4fbe2f-404c-4c5d-a4f9-5b1c06583c53"}
I1130 10:37:19.032075    5920 start.go:140] virtualization: kvm guest
I1130 10:37:19.033922    5920 out.go:179] üòÑ  minikube v1.37.0 on Ubuntu 24.04 (kvm/amd64)
I1130 10:37:19.035728    5920 notify.go:220] Checking for updates...
W1130 10:37:19.035747    5920 preload.go:293] Failed to list preload files: open /home/aakash/.minikube/cache/preloaded-tarball: no such file or directory
I1130 10:37:19.037510    5920 driver.go:421] Setting default libvirt URI to qemu:///system
I1130 10:37:19.037729    5920 global.go:112] Querying for installed drivers using PATH=/home/aakash/.minikube/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/wsl/lib:/mnt/c/Program Files/Common Files/Oracle/Java/javapath:/mnt/c/Program Files (x86)/Common Files/Oracle/Java/java8path:/mnt/c/Program Files (x86)/Common Files/Oracle/Java/javapath:/mnt/c/Windows/system32:/mnt/c/Windows:/mnt/c/Windows/System32/Wbem:/mnt/c/Windows/System32/WindowsPowerShell/v1.0/:/mnt/c/Windows/System32/OpenSSH/:/mnt/c/Program Files (x86)/NVIDIA Corporation/PhysX/Common:/mnt/c/Program Files/NVIDIA Corporation/NVIDIA NvDLISR:/mnt/c/Program Files/nodejs/:/mnt/c/ProgramData/chocolatey/bin:/mnt/c/Program Files/GitHub CLI/:/mnt/c/WINDOWS/system32:/mnt/c/WINDOWS:/mnt/c/WINDOWS/System32/Wbem:/mnt/c/WINDOWS/System32/WindowsPowerShell/v1.0/:/mnt/c/WINDOWS/System32/OpenSSH/:/mnt/c/Program Files/Java/jdk-21/bin:/mnt/c/Program Files/Docker:/Docker/host/bin:/mnt/c/Program Files/Vagrant/bin:/mnt/c/Program Files/Git/cmd:/mnt/c/Users/aakas/AppData/Local/Programs/Python/Python312/:/mnt/c/Users/aakas/AppData/Roaming/npm:/mnt/c/Users/aakas/AppData/Local/Programs/Microsoft VS Code/bin:/mnt/c/Program Files/Oracle/VirtualBox:/snap/bin
I1130 10:37:19.116302    5920 global.go:133] vmware default: false priority: 5, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "vmrun": executable file not found in $PATH Reason: Fix:Install vmrun Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
I1130 10:37:19.163850    5920 docker.go:123] docker version: linux-27.5.1:Docker Desktop 4.0.0 ()
I1130 10:37:19.163975    5920 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1130 10:37:19.823103    5920 info.go:266] docker info: {ID:d722c428-a211-4883-9a71-6951b452c3fe Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:11 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:84 OomKillDisable:false NGoroutines:120 SystemTime:2025-11-30 16:37:19.809401329 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:17 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:8170704896 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///var/run/docker-cli.sock] ExperimentalBuild:false ServerVersion:27.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb Expected:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb} RuncCommit:{ID:v1.1.12-0-g51d5e946 Expected:v1.1.12-0-g51d5e946} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:/usr/local/lib/docker/cli-plugins/docker-ai SchemaVersion:0.1.0 ShortDescription:Ask Gordon - Docker Agent Vendor:Docker Inc. Version:v0.7.3] map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.20.1-desktop.2] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.32.4-desktop.1] map[Name:debug Path:/usr/local/lib/docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:/usr/local/lib/docker/cli-plugins/docker-desktop SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.4] map[Name:dev Path:/usr/local/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:feedback Path:/usr/local/lib/docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:/usr/local/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/usr/local/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.16.1]] Warnings:<nil>}}
I1130 10:37:19.823211    5920 docker.go:318] overlay module found
I1130 10:37:19.823235    5920 global.go:133] docker default: true priority: 9, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1130 10:37:19.838345    5920 global.go:133] none default: false priority: 4, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:running the 'none' driver as a regular user requires sudo permissions Reason: Fix: Doc: Version:}
I1130 10:37:19.918819    5920 global.go:133] podman default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "podman": executable file not found in $PATH Reason: Fix:Install Podman Doc:https://minikube.sigs.k8s.io/docs/drivers/podman/ Version:}
I1130 10:37:19.918866    5920 global.go:133] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1130 10:37:19.965686    5920 global.go:133] kvm2 default: true priority: 8, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "virsh": executable file not found in $PATH Reason: Fix:Install libvirt Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/kvm2/ Version:}
I1130 10:37:20.040618    5920 global.go:133] qemu2 default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "qemu-system-x86_64": executable file not found in $PATH Reason: Fix:Install qemu-system Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/qemu/ Version:}
I1130 10:37:20.159731    5920 global.go:133] virtualbox default: true priority: 6, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:unable to find VBoxManage in $PATH Reason: Fix:Install VirtualBox Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/virtualbox/ Version:}
I1130 10:37:20.159787    5920 driver.go:343] not recommending "ssh" due to default: false
I1130 10:37:20.159801    5920 driver.go:378] Picked: docker
I1130 10:37:20.159805    5920 driver.go:379] Alternatives: [ssh]
I1130 10:37:20.159808    5920 driver.go:380] Rejects: [vmware none podman kvm2 qemu2 virtualbox]
I1130 10:37:20.161078    5920 out.go:179] ‚ú®  Automatically selected the docker driver
I1130 10:37:20.162997    5920 start.go:304] selected driver: docker
I1130 10:37:20.163056    5920 start.go:918] validating driver "docker" against <nil>
I1130 10:37:20.163069    5920 start.go:929] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1130 10:37:20.163148    5920 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1130 10:37:20.430942    5920 info.go:266] docker info: {ID:d722c428-a211-4883-9a71-6951b452c3fe Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:11 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:85 OomKillDisable:false NGoroutines:121 SystemTime:2025-11-30 16:37:20.41777318 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:17 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:8170704896 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///var/run/docker-cli.sock] ExperimentalBuild:false ServerVersion:27.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb Expected:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb} RuncCommit:{ID:v1.1.12-0-g51d5e946 Expected:v1.1.12-0-g51d5e946} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:/usr/local/lib/docker/cli-plugins/docker-ai SchemaVersion:0.1.0 ShortDescription:Ask Gordon - Docker Agent Vendor:Docker Inc. Version:v0.7.3] map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.20.1-desktop.2] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.32.4-desktop.1] map[Name:debug Path:/usr/local/lib/docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:/usr/local/lib/docker/cli-plugins/docker-desktop SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.4] map[Name:dev Path:/usr/local/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:feedback Path:/usr/local/lib/docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:/usr/local/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/usr/local/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.16.1]] Warnings:<nil>}}
I1130 10:37:20.431130    5920 start_flags.go:327] no existing cluster config was found, will generate one from the flags 
I1130 10:37:20.432105    5920 start_flags.go:410] Using suggested 3072MB memory alloc based on sys=7792MB, container=7792MB
I1130 10:37:20.432500    5920 start_flags.go:974] Wait components to verify : map[apiserver:true system_pods:true]
I1130 10:37:20.434370    5920 out.go:179] üìå  Using Docker driver with root privileges
W1130 10:37:20.435418    5920 out.go:285] ‚ùó  For an improved experience it's recommended to use Docker Engine instead of Docker Desktop.
Docker Engine installation instructions: https://docs.docker.com/engine/install/#server
I1130 10:37:20.435526    5920 cni.go:84] Creating CNI manager for ""
I1130 10:37:20.436103    5920 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1130 10:37:20.436110    5920 start_flags.go:336] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I1130 10:37:20.436668    5920 start.go:348] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3072 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1130 10:37:20.437840    5920 out.go:179] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I1130 10:37:20.439373    5920 cache.go:123] Beginning downloading kic base image for docker with docker
I1130 10:37:20.440299    5920 out.go:179] üöú  Pulling base image v0.0.48 ...
I1130 10:37:20.441141    5920 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1130 10:37:20.441683    5920 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local docker daemon
I1130 10:37:20.516723    5920 cache.go:152] Downloading gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 to local cache
I1130 10:37:20.518055    5920 image.go:65] Checking for gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local cache directory
I1130 10:37:20.518156    5920 preload.go:118] Found remote preload: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.34.0/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4
I1130 10:37:20.518164    5920 cache.go:58] Caching tarball of preloaded images
I1130 10:37:20.518272    5920 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1130 10:37:20.518397    5920 image.go:150] Writing gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 to local cache
I1130 10:37:20.519386    5920 out.go:179] üíæ  Downloading Kubernetes v1.34.0 preload ...
I1130 10:37:20.520356    5920 preload.go:236] getting checksum for preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4 ...
I1130 10:37:20.711306    5920 download.go:108] Downloading: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.34.0/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4?checksum=md5:994a4de1464928e89c992dfd0a962e35 -> /home/aakash/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4
I1130 10:37:32.363868    5920 preload.go:247] saving checksum for preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4 ...
I1130 10:37:32.363926    5920 preload.go:254] verifying checksum of /home/aakash/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4 ...
I1130 10:37:33.040924    5920 cache.go:61] Finished verifying existence of preloaded tar for v1.34.0 on docker
I1130 10:37:33.041223    5920 profile.go:143] Saving config to /home/aakash/.minikube/profiles/minikube/config.json ...
I1130 10:37:33.041244    5920 lock.go:35] WriteFile acquiring /home/aakash/.minikube/profiles/minikube/config.json: {Name:mk285dea09a55a8b3acf618f2b79dee4dbe6f509 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1130 10:37:36.047728    5920 cache.go:155] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 as a tarball
I1130 10:37:36.047769    5920 cache.go:165] Loading gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 from local cache
I1130 10:37:38.743047    5920 cache.go:167] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 from cached tarball
I1130 10:37:38.743345    5920 cache.go:232] Successfully downloaded all kic artifacts
I1130 10:37:38.743910    5920 start.go:360] acquireMachinesLock for minikube: {Name:mke53dfa85ade730a1742e72e5c763e0d89b1bcb Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1130 10:37:38.744041    5920 start.go:364] duration metric: took 101.911¬µs to acquireMachinesLock for "minikube"
I1130 10:37:38.744072    5920 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3072 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1130 10:37:38.744165    5920 start.go:125] createHost starting for "" (driver="docker")
I1130 10:37:38.746092    5920 out.go:252] üî•  Creating docker container (CPUs=2, Memory=3072MB) ...
I1130 10:37:38.746687    5920 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I1130 10:37:38.746781    5920 client.go:168] LocalClient.Create starting
I1130 10:37:38.747875    5920 main.go:141] libmachine: Creating CA: /home/aakash/.minikube/certs/ca.pem
I1130 10:37:38.811043    5920 main.go:141] libmachine: Creating client certificate: /home/aakash/.minikube/certs/cert.pem
I1130 10:37:38.900618    5920 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W1130 10:37:38.930782    5920 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I1130 10:37:38.930839    5920 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I1130 10:37:38.930852    5920 cli_runner.go:164] Run: docker network inspect minikube
W1130 10:37:38.949041    5920 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I1130 10:37:38.949055    5920 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I1130 10:37:38.949064    5920 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I1130 10:37:38.949123    5920 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1130 10:37:38.968242    5920 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc0013a7d30}
I1130 10:37:38.968291    5920 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I1130 10:37:38.968322    5920 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I1130 10:37:39.115191    5920 network_create.go:108] docker network minikube 192.168.49.0/24 created
I1130 10:37:39.115242    5920 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I1130 10:37:39.115329    5920 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I1130 10:37:39.135351    5920 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I1130 10:37:39.155465    5920 oci.go:103] Successfully created a docker volume minikube
I1130 10:37:39.155518    5920 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 -d /var/lib
I1130 10:37:39.747760    5920 oci.go:107] Successfully prepared a docker volume minikube
I1130 10:37:39.747819    5920 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1130 10:37:39.747848    5920 kic.go:194] Starting extracting preloaded images to volume ...
I1130 10:37:39.747922    5920 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /home/aakash/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 -I lz4 -xf /preloaded.tar -C /extractDir
I1130 10:37:42.818000    5920 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /home/aakash/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 -I lz4 -xf /preloaded.tar -C /extractDir: (3.069990497s)
I1130 10:37:42.818039    5920 kic.go:203] duration metric: took 3.070187413s to extract preloaded images to volume ...
W1130 10:37:42.818481    5920 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
W1130 10:37:42.818572    5920 oci.go:252] Your kernel does not support CPU cfs period/quota or the cgroup is not mounted.
I1130 10:37:42.818626    5920 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I1130 10:37:43.202929    5920 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=3072mb -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1
I1130 10:37:43.521163    5920 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I1130 10:37:43.554917    5920 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1130 10:37:43.580158    5920 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I1130 10:37:43.709155    5920 oci.go:144] the created container "minikube" has a running status.
I1130 10:37:43.709218    5920 kic.go:225] Creating ssh key for kic: /home/aakash/.minikube/machines/minikube/id_rsa...
I1130 10:37:43.762258    5920 kic_runner.go:191] docker (temp): /home/aakash/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I1130 10:37:43.795901    5920 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1130 10:37:43.819855    5920 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I1130 10:37:43.819862    5920 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I1130 10:37:43.912254    5920 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1130 10:37:43.935461    5920 machine.go:93] provisionDockerMachine start ...
I1130 10:37:43.935605    5920 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1130 10:37:43.956653    5920 main.go:141] libmachine: Using SSH client type: native
I1130 10:37:43.956812    5920 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 51464 <nil> <nil>}
I1130 10:37:43.956817    5920 main.go:141] libmachine: About to run SSH command:
hostname
I1130 10:37:44.079929    5920 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1130 10:37:44.080449    5920 ubuntu.go:182] provisioning hostname "minikube"
I1130 10:37:44.080514    5920 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1130 10:37:44.109090    5920 main.go:141] libmachine: Using SSH client type: native
I1130 10:37:44.109213    5920 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 51464 <nil> <nil>}
I1130 10:37:44.109220    5920 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1130 10:37:44.242046    5920 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1130 10:37:44.242241    5920 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1130 10:37:44.262545    5920 main.go:141] libmachine: Using SSH client type: native
I1130 10:37:44.262693    5920 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 51464 <nil> <nil>}
I1130 10:37:44.262702    5920 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1130 10:37:44.395720    5920 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1130 10:37:44.395848    5920 ubuntu.go:188] set auth options {CertDir:/home/aakash/.minikube CaCertPath:/home/aakash/.minikube/certs/ca.pem CaPrivateKeyPath:/home/aakash/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/aakash/.minikube/machines/server.pem ServerKeyPath:/home/aakash/.minikube/machines/server-key.pem ClientKeyPath:/home/aakash/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/aakash/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/aakash/.minikube}
I1130 10:37:44.395875    5920 ubuntu.go:190] setting up certificates
I1130 10:37:44.395890    5920 provision.go:84] configureAuth start
I1130 10:37:44.395961    5920 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1130 10:37:44.418631    5920 provision.go:143] copyHostCerts
I1130 10:37:44.418716    5920 exec_runner.go:151] cp: /home/aakash/.minikube/certs/ca.pem --> /home/aakash/.minikube/ca.pem (1078 bytes)
I1130 10:37:44.418803    5920 exec_runner.go:151] cp: /home/aakash/.minikube/certs/cert.pem --> /home/aakash/.minikube/cert.pem (1123 bytes)
I1130 10:37:44.418843    5920 exec_runner.go:151] cp: /home/aakash/.minikube/certs/key.pem --> /home/aakash/.minikube/key.pem (1675 bytes)
I1130 10:37:44.418873    5920 provision.go:117] generating server cert: /home/aakash/.minikube/machines/server.pem ca-key=/home/aakash/.minikube/certs/ca.pem private-key=/home/aakash/.minikube/certs/ca-key.pem org=aakash.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I1130 10:37:44.471188    5920 provision.go:177] copyRemoteCerts
I1130 10:37:44.471223    5920 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1130 10:37:44.471247    5920 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1130 10:37:44.493061    5920 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51464 SSHKeyPath:/home/aakash/.minikube/machines/minikube/id_rsa Username:docker}
I1130 10:37:44.586395    5920 ssh_runner.go:362] scp /home/aakash/.minikube/machines/server.pem --> /etc/docker/server.pem (1180 bytes)
I1130 10:37:44.610331    5920 ssh_runner.go:362] scp /home/aakash/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1130 10:37:44.631516    5920 ssh_runner.go:362] scp /home/aakash/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I1130 10:37:44.651881    5920 provision.go:87] duration metric: took 255.98011ms to configureAuth
I1130 10:37:44.651898    5920 ubuntu.go:206] setting minikube options for container-runtime
I1130 10:37:44.652053    5920 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1130 10:37:44.652089    5920 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1130 10:37:44.675495    5920 main.go:141] libmachine: Using SSH client type: native
I1130 10:37:44.675660    5920 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 51464 <nil> <nil>}
I1130 10:37:44.675665    5920 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1130 10:37:44.802793    5920 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1130 10:37:44.802806    5920 ubuntu.go:71] root file system type: overlay
I1130 10:37:44.803409    5920 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I1130 10:37:44.803501    5920 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1130 10:37:44.825372    5920 main.go:141] libmachine: Using SSH client type: native
I1130 10:37:44.825499    5920 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 51464 <nil> <nil>}
I1130 10:37:44.825536    5920 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 \
	-H fd:// --containerd=/run/containerd/containerd.sock \
	-H unix:///var/run/docker.sock \
	--default-ulimit=nofile=1048576:1048576 \
	--tlsverify \
	--tlscacert /etc/docker/ca.pem \
	--tlscert /etc/docker/server.pem \
	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1130 10:37:44.981145    5920 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target

I1130 10:37:44.981298    5920 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1130 10:37:45.006233    5920 main.go:141] libmachine: Using SSH client type: native
I1130 10:37:45.006369    5920 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 51464 <nil> <nil>}
I1130 10:37:45.006377    5920 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1130 10:37:46.734125    5920 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2025-09-03 20:55:49.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2025-11-30 16:37:44.974381243 +0000
@@ -9,23 +9,34 @@
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
 Restart=always
 
+
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
+
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I1130 10:37:46.734139    5920 machine.go:96] duration metric: took 2.798668782s to provisionDockerMachine
I1130 10:37:46.734147    5920 client.go:171] duration metric: took 7.987362129s to LocalClient.Create
I1130 10:37:46.734164    5920 start.go:167] duration metric: took 7.987497387s to libmachine.API.Create "minikube"
I1130 10:37:46.734458    5920 start.go:293] postStartSetup for "minikube" (driver="docker")
I1130 10:37:46.734469    5920 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1130 10:37:46.734525    5920 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1130 10:37:46.734549    5920 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1130 10:37:46.756772    5920 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51464 SSHKeyPath:/home/aakash/.minikube/machines/minikube/id_rsa Username:docker}
I1130 10:37:46.869838    5920 ssh_runner.go:195] Run: cat /etc/os-release
I1130 10:37:46.874813    5920 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1130 10:37:46.874825    5920 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1130 10:37:46.874830    5920 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1130 10:37:46.874834    5920 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I1130 10:37:46.874897    5920 filesync.go:126] Scanning /home/aakash/.minikube/addons for local assets ...
I1130 10:37:46.874953    5920 filesync.go:126] Scanning /home/aakash/.minikube/files for local assets ...
I1130 10:37:46.874964    5920 start.go:296] duration metric: took 140.499562ms for postStartSetup
I1130 10:37:46.875183    5920 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1130 10:37:46.905276    5920 profile.go:143] Saving config to /home/aakash/.minikube/profiles/minikube/config.json ...
I1130 10:37:46.905658    5920 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1130 10:37:46.905707    5920 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1130 10:37:46.941701    5920 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51464 SSHKeyPath:/home/aakash/.minikube/machines/minikube/id_rsa Username:docker}
I1130 10:37:47.052952    5920 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1130 10:37:47.058068    5920 start.go:128] duration metric: took 8.313864233s to createHost
I1130 10:37:47.058105    5920 start.go:83] releasing machines lock for "minikube", held for 8.314057096s
I1130 10:37:47.058147    5920 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1130 10:37:47.086978    5920 ssh_runner.go:195] Run: cat /version.json
I1130 10:37:47.087005    5920 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1130 10:37:47.087062    5920 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1130 10:37:47.087100    5920 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1130 10:37:47.114903    5920 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51464 SSHKeyPath:/home/aakash/.minikube/machines/minikube/id_rsa Username:docker}
I1130 10:37:47.116638    5920 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51464 SSHKeyPath:/home/aakash/.minikube/machines/minikube/id_rsa Username:docker}
I1130 10:37:47.452079    5920 ssh_runner.go:195] Run: systemctl --version
I1130 10:37:47.459620    5920 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1130 10:37:47.464561    5920 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I1130 10:37:47.485708    5920 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I1130 10:37:47.485747    5920 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1130 10:37:47.504829    5920 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I1130 10:37:47.504854    5920 start.go:495] detecting cgroup driver to use...
I1130 10:37:47.504909    5920 detect.go:190] detected "systemd" cgroup driver on host os
I1130 10:37:47.505356    5920 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1130 10:37:47.516867    5920 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10.1"|' /etc/containerd/config.toml"
I1130 10:37:47.524837    5920 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1130 10:37:47.532311    5920 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I1130 10:37:47.532343    5920 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I1130 10:37:47.539380    5920 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1130 10:37:47.546409    5920 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1130 10:37:47.553258    5920 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1130 10:37:47.560141    5920 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1130 10:37:47.566440    5920 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1130 10:37:47.573589    5920 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I1130 10:37:47.580716    5920 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I1130 10:37:47.588222    5920 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1130 10:37:47.594482    5920 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1130 10:37:47.600366    5920 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1130 10:37:47.658173    5920 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1130 10:37:47.757697    5920 start.go:495] detecting cgroup driver to use...
I1130 10:37:47.757725    5920 detect.go:190] detected "systemd" cgroup driver on host os
I1130 10:37:47.757826    5920 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1130 10:37:47.767955    5920 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1130 10:37:47.776313    5920 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I1130 10:37:47.788012    5920 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1130 10:37:47.796501    5920 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1130 10:37:47.805398    5920 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1130 10:37:47.818930    5920 ssh_runner.go:195] Run: which cri-dockerd
I1130 10:37:47.822308    5920 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1130 10:37:47.829742    5920 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (192 bytes)
I1130 10:37:47.844257    5920 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1130 10:37:47.914831    5920 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1130 10:37:47.951760    5920 docker.go:575] configuring docker to use "systemd" as cgroup driver...
I1130 10:37:47.951831    5920 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I1130 10:37:47.964406    5920 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I1130 10:37:47.972081    5920 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1130 10:37:48.031129    5920 ssh_runner.go:195] Run: sudo systemctl restart docker
I1130 10:37:49.352346    5920 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.321198453s)
I1130 10:37:49.352389    5920 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service docker
I1130 10:37:49.360327    5920 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I1130 10:37:49.368378    5920 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1130 10:37:49.376426    5920 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1130 10:37:49.432756    5920 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1130 10:37:49.475439    5920 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1130 10:37:49.556449    5920 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1130 10:37:49.597567    5920 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I1130 10:37:49.613504    5920 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1130 10:37:49.681581    5920 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I1130 10:37:49.838090    5920 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1130 10:37:49.846563    5920 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1130 10:37:49.846597    5920 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1130 10:37:49.851201    5920 start.go:563] Will wait 60s for crictl version
I1130 10:37:49.851254    5920 ssh_runner.go:195] Run: which crictl
I1130 10:37:49.854255    5920 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1130 10:37:49.928200    5920 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.4.0
RuntimeApiVersion:  v1
I1130 10:37:49.928282    5920 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1130 10:37:49.950629    5920 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1130 10:37:49.970811    5920 out.go:252] üê≥  Preparing Kubernetes v1.34.0 on Docker 28.4.0 ...
I1130 10:37:49.970948    5920 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1130 10:37:49.993247    5920 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I1130 10:37:49.996426    5920 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1130 10:37:50.005287    5920 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1130 10:37:50.027895    5920 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3072 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I1130 10:37:50.028010    5920 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1130 10:37:50.028051    5920 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1130 10:37:50.042994    5920 docker.go:691] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1130 10:37:50.043003    5920 docker.go:621] Images already preloaded, skipping extraction
I1130 10:37:50.043059    5920 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1130 10:37:50.057350    5920 docker.go:691] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1130 10:37:50.057415    5920 cache_images.go:85] Images are preloaded, skipping loading
I1130 10:37:50.057431    5920 kubeadm.go:926] updating node { 192.168.49.2 8443 v1.34.0 docker true true} ...
I1130 10:37:50.057643    5920 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.34.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I1130 10:37:50.057681    5920 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1130 10:37:50.105247    5920 cni.go:84] Creating CNI manager for ""
I1130 10:37:50.105261    5920 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1130 10:37:50.105276    5920 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I1130 10:37:50.105299    5920 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.34.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1130 10:37:50.105387    5920 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
kubernetesVersion: v1.34.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1130 10:37:50.105437    5920 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.34.0
I1130 10:37:50.112549    5920 binaries.go:44] Found k8s binaries, skipping transfer
I1130 10:37:50.112581    5920 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1130 10:37:50.118675    5920 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I1130 10:37:50.130560    5920 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1130 10:37:50.142933    5920 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2208 bytes)
I1130 10:37:50.156537    5920 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1130 10:37:50.159218    5920 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1130 10:37:50.168070    5920 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1130 10:37:50.222792    5920 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1130 10:37:50.267594    5920 certs.go:68] Setting up /home/aakash/.minikube/profiles/minikube for IP: 192.168.49.2
I1130 10:37:50.267617    5920 certs.go:194] generating shared ca certs ...
I1130 10:37:50.267637    5920 certs.go:226] acquiring lock for ca certs: {Name:mkbf9d8221855486e1a8f64996d3c12eab062c55 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1130 10:37:50.267726    5920 certs.go:240] generating "minikubeCA" ca cert: /home/aakash/.minikube/ca.key
I1130 10:37:50.407971    5920 crypto.go:156] Writing cert to /home/aakash/.minikube/ca.crt ...
I1130 10:37:50.407990    5920 lock.go:35] WriteFile acquiring /home/aakash/.minikube/ca.crt: {Name:mkc5580df76bf3b324c7e26da271cefa00c0b29c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1130 10:37:50.408155    5920 crypto.go:164] Writing key to /home/aakash/.minikube/ca.key ...
I1130 10:37:50.408160    5920 lock.go:35] WriteFile acquiring /home/aakash/.minikube/ca.key: {Name:mk2c147fb7878c52926ee3455a0f7ca16dd41319 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1130 10:37:50.408215    5920 certs.go:240] generating "proxyClientCA" ca cert: /home/aakash/.minikube/proxy-client-ca.key
I1130 10:37:50.421791    5920 crypto.go:156] Writing cert to /home/aakash/.minikube/proxy-client-ca.crt ...
I1130 10:37:50.421803    5920 lock.go:35] WriteFile acquiring /home/aakash/.minikube/proxy-client-ca.crt: {Name:mk4087b58f77821a723ad01f04308292b15fd539 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1130 10:37:50.421932    5920 crypto.go:164] Writing key to /home/aakash/.minikube/proxy-client-ca.key ...
I1130 10:37:50.421936    5920 lock.go:35] WriteFile acquiring /home/aakash/.minikube/proxy-client-ca.key: {Name:mk12a9193b63cd76b4970b5530424f7f0871998d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1130 10:37:50.421981    5920 certs.go:256] generating profile certs ...
I1130 10:37:50.422039    5920 certs.go:363] generating signed profile cert for "minikube-user": /home/aakash/.minikube/profiles/minikube/client.key
I1130 10:37:50.422046    5920 crypto.go:68] Generating cert /home/aakash/.minikube/profiles/minikube/client.crt with IP's: []
I1130 10:37:50.482167    5920 crypto.go:156] Writing cert to /home/aakash/.minikube/profiles/minikube/client.crt ...
I1130 10:37:50.482179    5920 lock.go:35] WriteFile acquiring /home/aakash/.minikube/profiles/minikube/client.crt: {Name:mk97258d044ffc2ec7dc915e11b0bcf7a6c7b4ec Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1130 10:37:50.482312    5920 crypto.go:164] Writing key to /home/aakash/.minikube/profiles/minikube/client.key ...
I1130 10:37:50.482315    5920 lock.go:35] WriteFile acquiring /home/aakash/.minikube/profiles/minikube/client.key: {Name:mk4c634d4b1d1f414e94bbd373b889c2eb6a5ee1 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1130 10:37:50.482358    5920 certs.go:363] generating signed profile cert for "minikube": /home/aakash/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I1130 10:37:50.482369    5920 crypto.go:68] Generating cert /home/aakash/.minikube/profiles/minikube/apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I1130 10:37:50.535637    5920 crypto.go:156] Writing cert to /home/aakash/.minikube/profiles/minikube/apiserver.crt.7fb57e3c ...
I1130 10:37:50.535648    5920 lock.go:35] WriteFile acquiring /home/aakash/.minikube/profiles/minikube/apiserver.crt.7fb57e3c: {Name:mkdaff9070f99b5d557fa55f30e0c268fdcdb76d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1130 10:37:50.535760    5920 crypto.go:164] Writing key to /home/aakash/.minikube/profiles/minikube/apiserver.key.7fb57e3c ...
I1130 10:37:50.535763    5920 lock.go:35] WriteFile acquiring /home/aakash/.minikube/profiles/minikube/apiserver.key.7fb57e3c: {Name:mkef2b05fa1dfc039e3fa62792a0d3c355582319 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1130 10:37:50.535807    5920 certs.go:381] copying /home/aakash/.minikube/profiles/minikube/apiserver.crt.7fb57e3c -> /home/aakash/.minikube/profiles/minikube/apiserver.crt
I1130 10:37:50.535955    5920 certs.go:385] copying /home/aakash/.minikube/profiles/minikube/apiserver.key.7fb57e3c -> /home/aakash/.minikube/profiles/minikube/apiserver.key
I1130 10:37:50.535997    5920 certs.go:363] generating signed profile cert for "aggregator": /home/aakash/.minikube/profiles/minikube/proxy-client.key
I1130 10:37:50.536006    5920 crypto.go:68] Generating cert /home/aakash/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I1130 10:37:50.650202    5920 crypto.go:156] Writing cert to /home/aakash/.minikube/profiles/minikube/proxy-client.crt ...
I1130 10:37:50.650214    5920 lock.go:35] WriteFile acquiring /home/aakash/.minikube/profiles/minikube/proxy-client.crt: {Name:mkaf05b61b55b39af885edfcfa0f1d06e6cbd247 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1130 10:37:50.650321    5920 crypto.go:164] Writing key to /home/aakash/.minikube/profiles/minikube/proxy-client.key ...
I1130 10:37:50.650324    5920 lock.go:35] WriteFile acquiring /home/aakash/.minikube/profiles/minikube/proxy-client.key: {Name:mk3cb7695ea23b0bfdd4d6ea6e02ab80d03b08eb Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1130 10:37:50.650454    5920 certs.go:484] found cert: /home/aakash/.minikube/certs/ca-key.pem (1679 bytes)
I1130 10:37:50.650469    5920 certs.go:484] found cert: /home/aakash/.minikube/certs/ca.pem (1078 bytes)
I1130 10:37:50.650481    5920 certs.go:484] found cert: /home/aakash/.minikube/certs/cert.pem (1123 bytes)
I1130 10:37:50.650490    5920 certs.go:484] found cert: /home/aakash/.minikube/certs/key.pem (1675 bytes)
I1130 10:37:50.650965    5920 ssh_runner.go:362] scp /home/aakash/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1130 10:37:50.668349    5920 ssh_runner.go:362] scp /home/aakash/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I1130 10:37:50.685040    5920 ssh_runner.go:362] scp /home/aakash/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1130 10:37:50.702770    5920 ssh_runner.go:362] scp /home/aakash/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1130 10:37:50.722092    5920 ssh_runner.go:362] scp /home/aakash/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I1130 10:37:50.740621    5920 ssh_runner.go:362] scp /home/aakash/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I1130 10:37:50.758329    5920 ssh_runner.go:362] scp /home/aakash/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1130 10:37:50.776094    5920 ssh_runner.go:362] scp /home/aakash/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I1130 10:37:50.793581    5920 ssh_runner.go:362] scp /home/aakash/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1130 10:37:50.813843    5920 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1130 10:37:50.827639    5920 ssh_runner.go:195] Run: openssl version
I1130 10:37:50.831827    5920 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1130 10:37:50.839974    5920 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1130 10:37:50.843417    5920 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Nov 30 16:37 /usr/share/ca-certificates/minikubeCA.pem
I1130 10:37:50.843449    5920 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1130 10:37:50.848256    5920 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1130 10:37:50.855160    5920 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I1130 10:37:50.858060    5920 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I1130 10:37:50.858186    5920 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3072 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1130 10:37:50.858268    5920 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1130 10:37:50.871822    5920 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1130 10:37:50.878732    5920 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1130 10:37:50.885268    5920 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I1130 10:37:50.885306    5920 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1130 10:37:50.891939    5920 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I1130 10:37:50.891945    5920 kubeadm.go:157] found existing configuration files:

I1130 10:37:50.891971    5920 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I1130 10:37:50.898239    5920 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I1130 10:37:50.898273    5920 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I1130 10:37:50.904780    5920 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I1130 10:37:50.911112    5920 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I1130 10:37:50.911148    5920 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I1130 10:37:50.917877    5920 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I1130 10:37:50.925179    5920 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I1130 10:37:50.925207    5920 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I1130 10:37:50.931247    5920 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I1130 10:37:50.937567    5920 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I1130 10:37:50.937600    5920 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I1130 10:37:50.943783    5920 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.34.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I1130 10:37:50.968259    5920 kubeadm.go:310] [init] Using Kubernetes version: v1.34.0
I1130 10:37:50.968334    5920 kubeadm.go:310] [preflight] Running pre-flight checks
I1130 10:37:51.064485    5920 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I1130 10:37:51.064546    5920 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I1130 10:37:51.064603    5920 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I1130 10:37:51.072605    5920 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I1130 10:37:51.074968    5920 out.go:252]     ‚ñ™ Generating certificates and keys ...
I1130 10:37:51.075066    5920 kubeadm.go:310] [certs] Using existing ca certificate authority
I1130 10:37:51.075098    5920 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I1130 10:37:51.171407    5920 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I1130 10:37:51.267613    5920 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I1130 10:37:51.360180    5920 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I1130 10:37:51.471257    5920 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I1130 10:37:51.853413    5920 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I1130 10:37:51.853478    5920 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I1130 10:37:51.924453    5920 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I1130 10:37:51.924539    5920 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I1130 10:37:52.001688    5920 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I1130 10:37:52.050353    5920 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I1130 10:37:52.108683    5920 kubeadm.go:310] [certs] Generating "sa" key and public key
I1130 10:37:52.108720    5920 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I1130 10:37:52.206779    5920 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I1130 10:37:52.263865    5920 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I1130 10:37:52.360630    5920 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I1130 10:37:52.396001    5920 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I1130 10:37:52.534239    5920 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I1130 10:37:52.534301    5920 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I1130 10:37:52.535358    5920 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I1130 10:37:52.554566    5920 out.go:252]     ‚ñ™ Booting up control plane ...
I1130 10:37:52.554684    5920 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I1130 10:37:52.554728    5920 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I1130 10:37:52.554765    5920 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I1130 10:37:52.579903    5920 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I1130 10:37:52.579984    5920 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/instance-config.yaml"
I1130 10:37:52.583467    5920 kubeadm.go:310] [patches] Applied patch of type "application/strategic-merge-patch+json" to target "kubeletconfiguration"
I1130 10:37:52.583769    5920 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I1130 10:37:52.583794    5920 kubeadm.go:310] [kubelet-start] Starting the kubelet
I1130 10:37:52.676983    5920 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I1130 10:37:52.677095    5920 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I1130 10:37:53.177368    5920 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 500.772321ms
I1130 10:37:53.180205    5920 kubeadm.go:310] [control-plane-check] Waiting for healthy control plane components. This can take up to 4m0s
I1130 10:37:53.180293    5920 kubeadm.go:310] [control-plane-check] Checking kube-apiserver at https://192.168.49.2:8443/livez
I1130 10:37:53.180399    5920 kubeadm.go:310] [control-plane-check] Checking kube-controller-manager at https://127.0.0.1:10257/healthz
I1130 10:37:53.180442    5920 kubeadm.go:310] [control-plane-check] Checking kube-scheduler at https://127.0.0.1:10259/livez
I1130 10:37:54.183171    5920 kubeadm.go:310] [control-plane-check] kube-controller-manager is healthy after 1.002703924s
I1130 10:37:54.829378    5920 kubeadm.go:310] [control-plane-check] kube-scheduler is healthy after 1.64898191s
I1130 10:37:56.688093    5920 kubeadm.go:310] [control-plane-check] kube-apiserver is healthy after 3.506747489s
I1130 10:37:56.722621    5920 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I1130 10:37:56.734328    5920 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I1130 10:37:56.739919    5920 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I1130 10:37:56.740030    5920 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I1130 10:37:56.744887    5920 kubeadm.go:310] [bootstrap-token] Using token: 9nbeuz.qw5u80s71dmhbeev
I1130 10:37:56.746143    5920 out.go:252]     ‚ñ™ Configuring RBAC rules ...
I1130 10:37:56.746533    5920 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I1130 10:37:56.748664    5920 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I1130 10:37:56.753585    5920 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I1130 10:37:56.755218    5920 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I1130 10:37:56.756871    5920 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I1130 10:37:56.758496    5920 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I1130 10:37:57.097929    5920 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I1130 10:37:57.548602    5920 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I1130 10:37:58.099236    5920 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I1130 10:37:58.100167    5920 kubeadm.go:310] 
I1130 10:37:58.100235    5920 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I1130 10:37:58.100239    5920 kubeadm.go:310] 
I1130 10:37:58.100354    5920 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I1130 10:37:58.100358    5920 kubeadm.go:310] 
I1130 10:37:58.100382    5920 kubeadm.go:310]   mkdir -p $HOME/.kube
I1130 10:37:58.100439    5920 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I1130 10:37:58.100487    5920 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I1130 10:37:58.100489    5920 kubeadm.go:310] 
I1130 10:37:58.100541    5920 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I1130 10:37:58.100544    5920 kubeadm.go:310] 
I1130 10:37:58.100623    5920 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I1130 10:37:58.100632    5920 kubeadm.go:310] 
I1130 10:37:58.100702    5920 kubeadm.go:310] You should now deploy a pod network to the cluster.
I1130 10:37:58.100871    5920 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I1130 10:37:58.100959    5920 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I1130 10:37:58.100963    5920 kubeadm.go:310] 
I1130 10:37:58.101042    5920 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I1130 10:37:58.101113    5920 kubeadm.go:310] and service account keys on each node and then running the following as root:
I1130 10:37:58.101116    5920 kubeadm.go:310] 
I1130 10:37:58.101216    5920 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token 9nbeuz.qw5u80s71dmhbeev \
I1130 10:37:58.101348    5920 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:da64709db44d836a867c3385f9ef6debaed683e7b78555346a8ec623cb7b94e5 \
I1130 10:37:58.101368    5920 kubeadm.go:310] 	--control-plane 
I1130 10:37:58.101371    5920 kubeadm.go:310] 
I1130 10:37:58.101453    5920 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I1130 10:37:58.101456    5920 kubeadm.go:310] 
I1130 10:37:58.101609    5920 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token 9nbeuz.qw5u80s71dmhbeev \
I1130 10:37:58.101741    5920 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:da64709db44d836a867c3385f9ef6debaed683e7b78555346a8ec623cb7b94e5 
I1130 10:37:58.104499    5920 kubeadm.go:310] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I1130 10:37:58.104582    5920 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I1130 10:37:58.104594    5920 cni.go:84] Creating CNI manager for ""
I1130 10:37:58.104604    5920 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1130 10:37:58.106562    5920 out.go:179] üîó  Configuring bridge CNI (Container Networking Interface) ...
I1130 10:37:58.108980    5920 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I1130 10:37:58.116382    5920 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I1130 10:37:58.128891    5920 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1130 10:37:58.129224    5920 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.34.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I1130 10:37:58.129237    5920 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_11_30T10_37_58_0700 minikube.k8s.io/version=v1.37.0 minikube.k8s.io/commit=65318f4cfff9c12cc87ec9eb8f4cdd57b25047f3 minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I1130 10:37:58.135822    5920 ops.go:34] apiserver oom_adj: -16
I1130 10:37:58.174144    5920 kubeadm.go:1105] duration metric: took 44.963577ms to wait for elevateKubeSystemPrivileges
I1130 10:37:58.188383    5920 kubeadm.go:394] duration metric: took 7.330219638s to StartCluster
I1130 10:37:58.188403    5920 settings.go:142] acquiring lock: {Name:mkee0521e6f6cec4ad4f5cdb05f1f643dfd3a5ca Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1130 10:37:58.188493    5920 settings.go:150] Updating kubeconfig:  /home/aakash/.kube/config
I1130 10:37:58.189331    5920 lock.go:35] WriteFile acquiring /home/aakash/.kube/config: {Name:mk33f66117c40188862125f0c2667ed190b55124 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1130 10:37:58.189600    5920 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1130 10:37:58.189653    5920 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1130 10:37:58.189726    5920 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1130 10:37:58.189747    5920 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubetail:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I1130 10:37:58.189824    5920 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1130 10:37:58.189830    5920 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1130 10:37:58.189874    5920 addons.go:238] Setting addon storage-provisioner=true in "minikube"
I1130 10:37:58.189919    5920 host.go:66] Checking if "minikube" exists ...
I1130 10:37:58.189977    5920 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1130 10:37:58.190183    5920 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1130 10:37:58.190197    5920 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1130 10:37:58.191286    5920 out.go:179] üîé  Verifying Kubernetes components...
I1130 10:37:58.193119    5920 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1130 10:37:58.226635    5920 out.go:179]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1130 10:37:58.227283    5920 addons.go:238] Setting addon default-storageclass=true in "minikube"
I1130 10:37:58.227301    5920 host.go:66] Checking if "minikube" exists ...
I1130 10:37:58.227483    5920 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1130 10:37:58.227884    5920 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1130 10:37:58.227893    5920 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1130 10:37:58.227963    5920 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1130 10:37:58.242981    5920 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.49.1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I1130 10:37:58.253131    5920 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51464 SSHKeyPath:/home/aakash/.minikube/machines/minikube/id_rsa Username:docker}
I1130 10:37:58.253424    5920 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I1130 10:37:58.253433    5920 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1130 10:37:58.253454    5920 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1130 10:37:58.253480    5920 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1130 10:37:58.277239    5920 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51464 SSHKeyPath:/home/aakash/.minikube/machines/minikube/id_rsa Username:docker}
I1130 10:37:58.331076    5920 start.go:976] {"host.minikube.internal": 192.168.49.1} host record injected into CoreDNS's ConfigMap
I1130 10:37:58.331178    5920 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1130 10:37:58.353472    5920 api_server.go:52] waiting for apiserver process to appear ...
I1130 10:37:58.353502    5920 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1130 10:37:58.354731    5920 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1130 10:37:58.361254    5920 api_server.go:72] duration metric: took 171.566177ms to wait for apiserver process to appear ...
I1130 10:37:58.361263    5920 api_server.go:88] waiting for apiserver healthz status ...
I1130 10:37:58.361273    5920 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51468/healthz ...
I1130 10:37:58.364220    5920 api_server.go:279] https://127.0.0.1:51468/healthz returned 200:
ok
I1130 10:37:58.365263    5920 api_server.go:141] control plane version: v1.34.0
I1130 10:37:58.365272    5920 api_server.go:131] duration metric: took 4.00621ms to wait for apiserver health ...
I1130 10:37:58.365334    5920 system_pods.go:43] waiting for kube-system pods to appear ...
I1130 10:37:58.367906    5920 system_pods.go:59] 4 kube-system pods found
I1130 10:37:58.367934    5920 system_pods.go:61] "etcd-minikube" [ea604532-9bcd-4464-b9e1-9d8abc31ff39] Pending
I1130 10:37:58.367937    5920 system_pods.go:61] "kube-apiserver-minikube" [4ae5838f-53b0-4775-a674-38de12c82ded] Pending
I1130 10:37:58.367939    5920 system_pods.go:61] "kube-controller-manager-minikube" [adcca93b-b838-4b65-9c84-2be3e42327fc] Pending
I1130 10:37:58.367941    5920 system_pods.go:61] "kube-scheduler-minikube" [9d8093cc-3291-4c46-9ec3-dfddd83bf749] Pending
I1130 10:37:58.367943    5920 system_pods.go:74] duration metric: took 2.60647ms to wait for pod list to return data ...
I1130 10:37:58.367949    5920 kubeadm.go:578] duration metric: took 178.264766ms to wait for: map[apiserver:true system_pods:true]
I1130 10:37:58.367974    5920 node_conditions.go:102] verifying NodePressure condition ...
I1130 10:37:58.370026    5920 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I1130 10:37:58.370056    5920 node_conditions.go:123] node cpu capacity is 16
I1130 10:37:58.370105    5920 node_conditions.go:105] duration metric: took 2.129504ms to run NodePressure ...
I1130 10:37:58.370113    5920 start.go:241] waiting for startup goroutines ...
I1130 10:37:58.374321    5920 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1130 10:37:58.518194    5920 out.go:179] üåü  Enabled addons: storage-provisioner, default-storageclass
I1130 10:37:58.519206    5920 addons.go:514] duration metric: took 329.496256ms for enable addons: enabled=[storage-provisioner default-storageclass]
I1130 10:37:58.837531    5920 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I1130 10:37:58.837593    5920 start.go:246] waiting for cluster config update ...
I1130 10:37:58.837636    5920 start.go:255] writing updated cluster config ...
I1130 10:37:58.838107    5920 ssh_runner.go:195] Run: rm -f paused
I1130 10:37:58.990299    5920 start.go:617] kubectl: 1.31.4, cluster: 1.34.0 (minor skew: 3)
I1130 10:37:59.005497    5920 out.go:203] 
W1130 10:37:59.006817    5920 out.go:285] ‚ùó  /usr/local/bin/kubectl is version 1.31.4, which may have incompatibilities with Kubernetes 1.34.0.
I1130 10:37:59.010377    5920 out.go:179]     ‚ñ™ Want kubectl v1.34.0? Try 'minikube kubectl -- get pods -A'
I1130 10:37:59.011495    5920 out.go:179] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Nov 30 16:37:49 minikube dockerd[1117]: time="2025-11-30T16:37:49.294984573Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint_count e514cf14913dbea70adc32d275315d7153d41007f82ff1fbef97cd8ac851c221], retrying...."
Nov 30 16:37:49 minikube dockerd[1117]: time="2025-11-30T16:37:49.325380245Z" level=info msg="Loading containers: done."
Nov 30 16:37:49 minikube dockerd[1117]: time="2025-11-30T16:37:49.333196087Z" level=info msg="Docker daemon" commit=249d679 containerd-snapshotter=false storage-driver=overlay2 version=28.4.0
Nov 30 16:37:49 minikube dockerd[1117]: time="2025-11-30T16:37:49.333241912Z" level=info msg="Initializing buildkit"
Nov 30 16:37:49 minikube dockerd[1117]: time="2025-11-30T16:37:49.346746397Z" level=info msg="Completed buildkit initialization"
Nov 30 16:37:49 minikube dockerd[1117]: time="2025-11-30T16:37:49.350838220Z" level=info msg="Daemon has completed initialization"
Nov 30 16:37:49 minikube dockerd[1117]: time="2025-11-30T16:37:49.350896188Z" level=info msg="API listen on /var/run/docker.sock"
Nov 30 16:37:49 minikube dockerd[1117]: time="2025-11-30T16:37:49.350934359Z" level=info msg="API listen on /run/docker.sock"
Nov 30 16:37:49 minikube dockerd[1117]: time="2025-11-30T16:37:49.350936502Z" level=info msg="API listen on [::]:2376"
Nov 30 16:37:49 minikube systemd[1]: Started Docker Application Container Engine.
Nov 30 16:37:49 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Nov 30 16:37:49 minikube cri-dockerd[1426]: time="2025-11-30T16:37:49Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Nov 30 16:37:49 minikube cri-dockerd[1426]: time="2025-11-30T16:37:49Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Nov 30 16:37:49 minikube cri-dockerd[1426]: time="2025-11-30T16:37:49Z" level=info msg="Start docker client with request timeout 0s"
Nov 30 16:37:49 minikube cri-dockerd[1426]: time="2025-11-30T16:37:49Z" level=info msg="Hairpin mode is set to hairpin-veth"
Nov 30 16:37:49 minikube cri-dockerd[1426]: time="2025-11-30T16:37:49Z" level=info msg="Loaded network plugin cni"
Nov 30 16:37:49 minikube cri-dockerd[1426]: time="2025-11-30T16:37:49Z" level=info msg="Docker cri networking managed by network plugin cni"
Nov 30 16:37:49 minikube cri-dockerd[1426]: time="2025-11-30T16:37:49Z" level=info msg="Setting cgroupDriver systemd"
Nov 30 16:37:49 minikube cri-dockerd[1426]: time="2025-11-30T16:37:49Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Nov 30 16:37:49 minikube cri-dockerd[1426]: time="2025-11-30T16:37:49Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Nov 30 16:37:49 minikube cri-dockerd[1426]: time="2025-11-30T16:37:49Z" level=info msg="Start cri-dockerd grpc backend"
Nov 30 16:37:49 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Nov 30 16:37:53 minikube cri-dockerd[1426]: time="2025-11-30T16:37:53Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7a0db2fa99dec8d5d718c5ef5faa8a95fae0c2ad0a926c7e7b01d969b3d4d9be/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 30 16:37:53 minikube cri-dockerd[1426]: time="2025-11-30T16:37:53Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/db4784d0e04ac25b08ca0b3fa63ff411520a78085789700d4077251d75cc6777/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 30 16:37:53 minikube cri-dockerd[1426]: time="2025-11-30T16:37:53Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/af8fb320deb2d36a0ed87da6ea38444ef628a6fc352a9e1152119f3b0f3f6149/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 30 16:37:53 minikube cri-dockerd[1426]: time="2025-11-30T16:37:53Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/50477a5a76b83c22b361d76189667cec378e6e255d3ea74d1f0bde127335be6c/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 30 16:38:03 minikube cri-dockerd[1426]: time="2025-11-30T16:38:03Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/16e4e2c0e75dac35d721a256d0003e1e463e0f21d36b7361f46f39e5ae086fa8/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 30 16:38:03 minikube cri-dockerd[1426]: time="2025-11-30T16:38:03Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/38216ff146757ef1bf1666aba1550ee63b00676c6f20f971ced1f6fa39895747/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 30 16:38:03 minikube cri-dockerd[1426]: time="2025-11-30T16:38:03Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3a9e5bea9055f3b2c7b79e6b0e217df1a38622f1c897712e301d4fff614188b1/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 30 16:38:06 minikube cri-dockerd[1426]: time="2025-11-30T16:38:06Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Nov 30 16:38:32 minikube cri-dockerd[1426]: time="2025-11-30T16:38:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1335025c1ca5f8d52900419850f7e9c2a9a4bd351a020cb9bef4807a4fecd29c/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 30 16:38:32 minikube cri-dockerd[1426]: time="2025-11-30T16:38:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6f4b6c448ab1fbd468d7f2e5f38d032d1b459c4bd1ace68f5721e9fb151f598f/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 30 16:38:32 minikube cri-dockerd[1426]: time="2025-11-30T16:38:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1be61529aa37f1dd7dd38c08be3faf49d7dca617578d3a3bb1cc5ea22f5cf4c7/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 30 16:38:32 minikube cri-dockerd[1426]: time="2025-11-30T16:38:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/47a4ef3d51cf48c73d166daf965749127feff41fe25bf42c4a68f9e644170789/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 30 16:38:32 minikube cri-dockerd[1426]: time="2025-11-30T16:38:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ee21a1f222e542f882cd5883496f2dd59c5cfc7a0a80defb1d5e108076ea1daf/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 30 16:38:32 minikube cri-dockerd[1426]: time="2025-11-30T16:38:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0c379344c2ae256ea2d7ca3be973e7680fb6d4e2ec8becaa29c104365e9b0675/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 30 16:38:32 minikube cri-dockerd[1426]: time="2025-11-30T16:38:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/434c0574d9e9be67e3a8939a22644e372f7bfc4b6bb20b53e7875b2d41a1afc8/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 30 16:38:32 minikube cri-dockerd[1426]: time="2025-11-30T16:38:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3fe2e7001a9aba9ac808fb0784bb08dc34481fd73519dd9b3430980fb60d5410/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 30 16:38:32 minikube cri-dockerd[1426]: time="2025-11-30T16:38:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/db51ca4b3f6aed349aaaae921a966c7f229ffeb5c62810bd249398c3e31e96ce/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 30 16:38:33 minikube cri-dockerd[1426]: time="2025-11-30T16:38:33Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3bde9ab6155b670f513f340364fc000a73ef65c163f39a01e310055dc53048d9/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 30 16:38:33 minikube cri-dockerd[1426]: time="2025-11-30T16:38:33Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f693434ccbc9501bcc685fde99e49f4cc9fa6a9daae41db96ca6942da20322f4/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 30 16:38:38 minikube dockerd[1117]: time="2025-11-30T16:38:38.634374238Z" level=info msg="ignoring event" container=cbcddd7bbc9c11dcb4179ea1e5a352fa20ab6d1ad38414490b5dbbe96acaa13a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 16:38:38 minikube dockerd[1117]: time="2025-11-30T16:38:38.978258891Z" level=info msg="ignoring event" container=0c379344c2ae256ea2d7ca3be973e7680fb6d4e2ec8becaa29c104365e9b0675 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 16:38:39 minikube cri-dockerd[1426]: time="2025-11-30T16:38:39Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/bf259e8f078ad20cbbda2b5c42a2d72b9b1b122bc4e740c00d8d1f53cf703028/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 30 16:38:39 minikube dockerd[1117]: time="2025-11-30T16:38:39.500664743Z" level=info msg="ignoring event" container=e63c00ade388d5a0c301ba57f954764b6b649841d68d31b3e5698b8baaa6c9cf module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 16:38:39 minikube dockerd[1117]: time="2025-11-30T16:38:39.823739357Z" level=info msg="ignoring event" container=3fe2e7001a9aba9ac808fb0784bb08dc34481fd73519dd9b3430980fb60d5410 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 16:38:39 minikube cri-dockerd[1426]: time="2025-11-30T16:38:39Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4b08f4f1e87d480a0a1acd78bcbf89f59c564a1d99607461fb42e92f5c690b7e/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 30 16:38:40 minikube cri-dockerd[1426]: time="2025-11-30T16:38:40Z" level=info msg="Stop pulling image mongo:4.4.18: Status: Downloaded newer image for mongo:4.4.18"
Nov 30 16:38:44 minikube cri-dockerd[1426]: time="2025-11-30T16:38:44Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/433a131a887d6431cfd574a14d6f6a3c95e6b9e0063d90118081e2b80b04594e/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 30 16:38:45 minikube dockerd[1117]: time="2025-11-30T16:38:45.067080671Z" level=info msg="ignoring event" container=487194d73e433bf967501f07b3cfcc37442b9a101ee378d843ead40d824247e4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 16:38:45 minikube dockerd[1117]: time="2025-11-30T16:38:45.339245930Z" level=info msg="ignoring event" container=ee21a1f222e542f882cd5883496f2dd59c5cfc7a0a80defb1d5e108076ea1daf module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 16:38:45 minikube dockerd[1117]: time="2025-11-30T16:38:45.773116156Z" level=info msg="ignoring event" container=9ce5fc80a327e17e2a7add0b1e442917e7e94ad7117d27e98ed165eefed4b412 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 16:38:46 minikube dockerd[1117]: time="2025-11-30T16:38:46.060592997Z" level=info msg="ignoring event" container=434c0574d9e9be67e3a8939a22644e372f7bfc4b6bb20b53e7875b2d41a1afc8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 16:38:46 minikube dockerd[1117]: time="2025-11-30T16:38:46.546709492Z" level=info msg="ignoring event" container=fee293e20ec44f7be600fa5e5cd77ea632f4e5e36178e20aaa8e3d7d6bf4ba75 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 16:38:46 minikube dockerd[1117]: time="2025-11-30T16:38:46.912131747Z" level=info msg="ignoring event" container=1335025c1ca5f8d52900419850f7e9c2a9a4bd351a020cb9bef4807a4fecd29c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 16:38:46 minikube cri-dockerd[1426]: time="2025-11-30T16:38:46Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5c8430417cb1ed6dc067a206125ca7372f3e5c3a1c11c8a74f06b3addf619702/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 30 16:38:53 minikube dockerd[1117]: time="2025-11-30T16:38:53.652330823Z" level=info msg="ignoring event" container=b0a6460701751d1b458aa9795b7428f49377840c6255a6c051045b45515085ea module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 16:38:53 minikube dockerd[1117]: time="2025-11-30T16:38:53.980641479Z" level=info msg="ignoring event" container=6f4b6c448ab1fbd468d7f2e5f38d032d1b459c4bd1ace68f5721e9fb151f598f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 16:38:57 minikube dockerd[1117]: time="2025-11-30T16:38:57.165400120Z" level=info msg="ignoring event" container=68350536d2a47202319dbff09351af8f1b0d8da1937c3fa0ec141c7923bd014b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 16:38:57 minikube dockerd[1117]: time="2025-11-30T16:38:57.425045243Z" level=info msg="ignoring event" container=47a4ef3d51cf48c73d166daf965749127feff41fe25bf42c4a68f9e644170789 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"


==> container status <==
CONTAINER           IMAGE                                                                           CREATED              STATE               NAME                      ATTEMPT             POD ID              POD
2537aea0e4000       04ef6c30f6629                                                                   41 seconds ago       Running             backend                   0                   5c8430417cb1e       backend-5f6469d4f-7hlhq
1189b9a290368       46ff9e6b966a2                                                                   43 seconds ago       Running             transactions              0                   433a131a887d6       transactions-dbdc64dd4-hh76s
e0f3a39368b68       mongo@sha256:d23ec07162ca06646a6329c452643f37494af644d045c002a7b41873981c160d   47 seconds ago       Running             mongo                     0                   1be61529aa37f       mongo-0
0709ac844ec07       04ef6c30f6629                                                                   48 seconds ago       Running             backend                   0                   4b08f4f1e87d4       backend-5f6469d4f-2495h
8607b6d404a21       354afb7535b0f                                                                   48 seconds ago       Running             studentportfolio          0                   bf259e8f078ad       studentportfolio-65ff89998c-qv4sb
2fbc3715c490f       354afb7535b0f                                                                   54 seconds ago       Running             studentportfolio          0                   f693434ccbc95       studentportfolio-65ff89998c-n677s
995e2eaf676bc       46ff9e6b966a2                                                                   54 seconds ago       Running             transactions              0                   3bde9ab6155b6       transactions-dbdc64dd4-69zs6
05a4348c4512b       04ef6c30f6629                                                                   55 seconds ago       Running             backend                   0                   db51ca4b3f6ae       backend-5f6469d4f-45xf7
cb3350eb53eb3       6e38f40d628db                                                                   About a minute ago   Running             storage-provisioner       0                   3a9e5bea9055f       storage-provisioner
ec0a0ce07188e       52546a367cc9e                                                                   About a minute ago   Running             coredns                   0                   16e4e2c0e75da       coredns-66bc5c9577-5krts
b0be3d891d540       df0860106674d                                                                   About a minute ago   Running             kube-proxy                0                   38216ff146757       kube-proxy-7btdq
5d3154e6f15c7       46169d968e920                                                                   About a minute ago   Running             kube-scheduler            0                   50477a5a76b83       kube-scheduler-minikube
a008fc3e7e1a4       5f1f5298c888d                                                                   About a minute ago   Running             etcd                      0                   af8fb320deb2d       etcd-minikube
b5be2b9a09141       a0af72f2ec6d6                                                                   About a minute ago   Running             kube-controller-manager   0                   7a0db2fa99dec       kube-controller-manager-minikube
1b58ab83f3dc7       90550c43ad2bc                                                                   About a minute ago   Running             kube-apiserver            0                   db4784d0e04ac       kube-apiserver-minikube


==> coredns [ec0a0ce07188] <==
[INFO] 10.244.0.12:33300 - 26251 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000154995s
[INFO] 10.244.0.12:33300 - 26441 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.00019083s
[INFO] 10.244.0.16:34895 - 22289 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000138789s
[INFO] 10.244.0.16:34895 - 22548 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000212764s
[INFO] 10.244.0.16:48730 - 35528 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000362644s
[INFO] 10.244.0.16:48730 - 34912 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000452704s
[INFO] 10.244.0.16:49229 - 32576 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000132461s
[INFO] 10.244.0.16:49229 - 32982 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000201823s
[INFO] 10.244.0.6:48824 - 17563 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000683189s
[INFO] 10.244.0.6:48824 - 16001 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000458963s
[INFO] 10.244.0.12:46526 - 49048 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000147903s
[INFO] 10.244.0.12:46526 - 49232 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000220779s
[INFO] 10.244.0.16:36774 - 54741 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000499047s
[INFO] 10.244.0.16:36774 - 53685 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000665106s
[INFO] 10.244.0.6:56983 - 58408 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000732625s
[INFO] 10.244.0.6:56983 - 57532 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.001022715s
[INFO] 10.244.0.12:43611 - 63134 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000389684s
[INFO] 10.244.0.12:43611 - 62489 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000619278s
[INFO] 10.244.0.16:53302 - 53851 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000145128s
[INFO] 10.244.0.16:53302 - 53673 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000193071s
[INFO] 10.244.0.16:36038 - 829 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000561972s
[INFO] 10.244.0.16:36038 - 65498 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000542219s
[INFO] 10.244.0.16:39904 - 28214 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.001166361s
[INFO] 10.244.0.16:39904 - 27544 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.001509301s
[INFO] 10.244.0.6:48686 - 59033 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.00014755s
[INFO] 10.244.0.6:48686 - 58728 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000223754s
[INFO] 10.244.0.12:59508 - 8111 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000304007s
[INFO] 10.244.0.12:59508 - 7777 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000378386s
[INFO] 10.244.0.16:45547 - 15751 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000135869s
[INFO] 10.244.0.16:45547 - 15600 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000181961s
[INFO] 10.244.0.6:53827 - 16621 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000849569s
[INFO] 10.244.0.6:53827 - 14901 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.001254146s
[INFO] 10.244.0.12:54523 - 14907 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000810857s
[INFO] 10.244.0.12:54523 - 14095 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000686038s
[INFO] 10.244.0.16:57975 - 49483 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000737637s
[INFO] 10.244.0.16:57975 - 48449 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.001440923s
[INFO] 10.244.0.16:47599 - 41659 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000790714s
[INFO] 10.244.0.16:47599 - 43606 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.001458992s
[INFO] 10.244.0.6:34365 - 5578 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000300733s
[INFO] 10.244.0.6:34365 - 5015 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000205545s
[INFO] 10.244.0.12:35231 - 6771 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000361086s
[INFO] 10.244.0.12:35231 - 6226 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000435242s
[INFO] 10.244.0.6:53219 - 16818 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000295014s
[INFO] 10.244.0.6:53219 - 16313 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000333708s
[INFO] 10.244.0.12:35488 - 20291 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000223599s
[INFO] 10.244.0.12:35488 - 19930 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000317013s
[INFO] 10.244.0.16:34056 - 25305 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000228268s
[INFO] 10.244.0.16:34056 - 25041 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.001005487s
[INFO] 10.244.0.16:60053 - 23922 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000269274s
[INFO] 10.244.0.16:60053 - 23530 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000343487s
[INFO] 10.244.0.6:44840 - 14816 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.00015207s
[INFO] 10.244.0.6:44840 - 14564 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000114742s
[INFO] 10.244.0.12:49530 - 49513 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000238642s
[INFO] 10.244.0.12:49530 - 49261 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000255302s
[INFO] 10.244.0.16:42571 - 30240 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000222646s
[INFO] 10.244.0.16:42571 - 29901 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000275444s
[INFO] 10.244.0.12:50397 - 40357 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000144513s
[INFO] 10.244.0.12:50397 - 40177 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000229163s
[INFO] 10.244.0.16:47482 - 36208 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000168255s
[INFO] 10.244.0.16:47482 - 36009 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000207846s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=65318f4cfff9c12cc87ec9eb8f4cdd57b25047f3
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_11_30T10_37_58_0700
                    minikube.k8s.io/version=v1.37.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 30 Nov 2025 16:37:54 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sun, 30 Nov 2025 16:39:26 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sun, 30 Nov 2025 16:38:56 +0000   Sun, 30 Nov 2025 16:37:54 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sun, 30 Nov 2025 16:38:56 +0000   Sun, 30 Nov 2025 16:37:54 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sun, 30 Nov 2025 16:38:56 +0000   Sun, 30 Nov 2025 16:37:54 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sun, 30 Nov 2025 16:38:56 +0000   Sun, 30 Nov 2025 16:37:55 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                16
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7979204Ki
  pods:               110
Allocatable:
  cpu:                16
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7979204Ki
  pods:               110
System Info:
  Machine ID:                 050776661d4d4a2998d3bcaae2c48ecb
  System UUID:                050776661d4d4a2998d3bcaae2c48ecb
  Boot ID:                    6fcef796-dd15-47a3-9a84-d6d7c276bf28
  Kernel Version:             6.6.87.2-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.4.0
  Kubelet Version:            v1.34.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (18 in total)
  Namespace                   Name                                 CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                 ------------  ----------  ---------------  -------------  ---
  default                     backend-5f6469d4f-2495h              100m (0%)     500m (3%)   128Mi (1%)       512Mi (6%)     48s
  default                     backend-5f6469d4f-45xf7              100m (0%)     500m (3%)   128Mi (1%)       512Mi (6%)     55s
  default                     backend-5f6469d4f-7hlhq              100m (0%)     500m (3%)   128Mi (1%)       512Mi (6%)     41s
  default                     mongo-0                              100m (0%)     500m (3%)   256Mi (3%)       1Gi (13%)      55s
  default                     nginx-5885db8fdb-6zcsk               0 (0%)        0 (0%)      0 (0%)           0 (0%)         55s
  default                     nginx-5885db8fdb-cjgn9               0 (0%)        0 (0%)      0 (0%)           0 (0%)         55s
  default                     nginx-6cf6b94dd4-5sw7n               0 (0%)        0 (0%)      0 (0%)           0 (0%)         54s
  default                     studentportfolio-65ff89998c-n677s    50m (0%)      200m (1%)   64Mi (0%)        256Mi (3%)     55s
  default                     studentportfolio-65ff89998c-qv4sb    50m (0%)      200m (1%)   64Mi (0%)        256Mi (3%)     49s
  default                     transactions-dbdc64dd4-69zs6         100m (0%)     500m (3%)   128Mi (1%)       512Mi (6%)     55s
  default                     transactions-dbdc64dd4-hh76s         100m (0%)     500m (3%)   128Mi (1%)       512Mi (6%)     43s
  kube-system                 coredns-66bc5c9577-5krts             100m (0%)     0 (0%)      70Mi (0%)        170Mi (2%)     84s
  kube-system                 etcd-minikube                        100m (0%)     0 (0%)      100Mi (1%)       0 (0%)         90s
  kube-system                 kube-apiserver-minikube              250m (1%)     0 (0%)      0 (0%)           0 (0%)         90s
  kube-system                 kube-controller-manager-minikube     200m (1%)     0 (0%)      0 (0%)           0 (0%)         90s
  kube-system                 kube-proxy-7btdq                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         85s
  kube-system                 kube-scheduler-minikube              100m (0%)     0 (0%)      0 (0%)           0 (0%)         90s
  kube-system                 storage-provisioner                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         89s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests      Limits
  --------           --------      ------
  cpu                1450m (9%)    3400m (21%)
  memory             1194Mi (15%)  4266Mi (54%)
  ephemeral-storage  0 (0%)        0 (0%)
  hugepages-1Gi      0 (0%)        0 (0%)
  hugepages-2Mi      0 (0%)        0 (0%)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 83s                kube-proxy       
  Normal  NodeHasSufficientMemory  94s (x8 over 94s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    94s (x8 over 94s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     94s (x7 over 94s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  94s                kubelet          Updated Node Allocatable limit across pods
  Normal  Starting                 90s                kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  90s                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  90s                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    90s                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     90s                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           86s                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Nov30 14:56] Hyper-V: Disabling IBT because of Hyper-V bug
[  +0.020444] PCI: Fatal: No config space access function found
[  +0.023761] PCI: System does not support PCI
[  +0.066576] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +1.087232] pulseaudio[296]: memfd_create() called without MFD_EXEC or MFD_NOEXEC_SEAL set
[  +0.051851] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.001891] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000324] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000300] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000431] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.005013] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000436] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000376] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000667] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.154868] systemd-journald[39]: File /var/log/journal/cc4fbe2f404c4c5da4f95b1c06583c53/system.journal corrupted or uncleanly shut down, renaming and replacing.
[  +0.253773] systemd-journald[39]: Failed to read journal file /var/log/journal/cc4fbe2f404c4c5da4f95b1c06583c53/user-1000.journal for rotation, trying to move it out of the way: Device or resource busy
[  +4.238858] WSL (259) ERROR: CheckConnection: getaddrinfo() failed: -5
[  +3.468285] WSL (1 - init(docker-desktop)) ERROR: ConfigApplyWindowsLibPath:2058: open /etc/ld.so.conf.d/ld.wsl.conf failed 2
[  +0.006189] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/America/Winnipeg not found. Is the tzdata package installed?
[  +0.070788] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.001438] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000529] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000438] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000537] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.004038] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000365] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000349] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000410] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.451478] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.003187] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000647] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000650] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000549] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.003084] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000657] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001570] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.002296] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.430353] systemd-journald[39]: File /var/log/journal/880e5af6b1eb43a4a71a06c3fae2123a/system.journal corrupted or uncleanly shut down, renaming and replacing.
[  +0.447449] netlink: 'init': attribute type 4 has an invalid length.
[  +1.460240] WSL (2 - Interop) ERROR: operator():2721: systemctl is-active user@1000.service returned: failed

[  +0.403888] WSL (2 - Interop) ERROR: operator():2721: systemctl is-active user@0.service returned: failed

[Nov30 15:06] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000071] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.309285] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000003] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.199490] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000004] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.560078] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000003] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.271309] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000005] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.195326] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000004] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.


==> etcd [a008fc3e7e1a] <==
{"level":"warn","ts":"2025-11-30T16:37:54.283616Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48524","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.285113Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48542","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.294125Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48562","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.297377Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48570","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.302003Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48606","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.310187Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48628","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.313637Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48644","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.316647Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48660","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.319921Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48670","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.322905Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48712","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.326236Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48722","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.329445Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48746","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.332549Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48762","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.335581Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48798","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.339164Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48814","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.342597Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48830","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.345761Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48838","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.348689Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48848","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.351657Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48872","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.354470Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48888","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.357381Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48918","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.360333Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48956","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.376186Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48962","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.379561Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:48984","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.382713Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49006","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.385789Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49012","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.388588Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49040","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.394062Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49048","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.395654Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49062","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.399091Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49090","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.402548Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49104","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.412238Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49120","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.415251Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49126","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.418177Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49144","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.421906Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49150","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.425061Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49172","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.428097Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49182","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.432482Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49192","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.435725Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49206","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.438612Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49210","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.441722Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49222","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.444807Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49254","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.447641Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49256","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.450435Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49272","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.453674Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49284","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.481174Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49310","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.484671Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49322","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.488006Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49336","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.491022Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49362","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.494108Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49386","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.501100Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49396","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.505297Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49412","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.508431Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49422","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.511201Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49430","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.514222Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49450","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.517255Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49466","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.552325Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49490","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.556552Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49510","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.559874Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49520","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T16:37:54.585383Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:49566","server-name":"","error":"EOF"}


==> kernel <==
 16:39:27 up  1:43,  0 users,  load average: 1.22, 0.71, 0.43
Linux minikube 6.6.87.2-microsoft-standard-WSL2 #1 SMP PREEMPT_DYNAMIC Thu Jun  5 18:30:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [1b58ab83f3dc] <==
I1130 16:37:54.797193       1 customresource_discovery_controller.go:294] Starting DiscoveryController
I1130 16:37:54.797377       1 naming_controller.go:299] Starting NamingConditionController
I1130 16:37:54.797406       1 controller.go:142] Starting OpenAPI controller
I1130 16:37:54.797417       1 controller.go:90] Starting OpenAPI V3 controller
I1130 16:37:54.797429       1 crd_finalizer.go:269] Starting CRDFinalizer
I1130 16:37:54.797435       1 establishing_controller.go:81] Starting EstablishingController
I1130 16:37:54.797442       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I1130 16:37:54.797448       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
E1130 16:37:54.857787       1 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
I1130 16:37:54.884491       1 shared_informer.go:356] "Caches are synced" controller="node_authorizer"
I1130 16:37:54.890279       1 shared_informer.go:356] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I1130 16:37:54.890334       1 policy_source.go:240] refreshing policies
I1130 16:37:54.896801       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1130 16:37:54.896949       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I1130 16:37:54.897006       1 cache.go:39] Caches are synced for LocalAvailability controller
I1130 16:37:54.897047       1 shared_informer.go:356] "Caches are synced" controller="kubernetes-service-cidr-controller"
I1130 16:37:54.897057       1 shared_informer.go:356] "Caches are synced" controller="cluster_authentication_trust_controller"
I1130 16:37:54.897077       1 shared_informer.go:356] "Caches are synced" controller="ipallocator-repair-controller"
I1130 16:37:54.897075       1 default_servicecidr_controller.go:166] Creating default ServiceCIDR with CIDRs: [10.96.0.0/12]
I1130 16:37:54.897119       1 apf_controller.go:382] Running API Priority and Fairness config worker
I1130 16:37:54.897134       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I1130 16:37:54.897144       1 cache.go:39] Caches are synced for RemoteAvailability controller
I1130 16:37:54.897243       1 shared_informer.go:356] "Caches are synced" controller="crd-autoregister"
I1130 16:37:54.897326       1 aggregator.go:171] initial CRD sync complete...
I1130 16:37:54.897368       1 autoregister_controller.go:144] Starting autoregister controller
I1130 16:37:54.897376       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1130 16:37:54.897384       1 cache.go:39] Caches are synced for autoregister controller
I1130 16:37:54.897393       1 shared_informer.go:356] "Caches are synced" controller="configmaps"
I1130 16:37:54.899838       1 controller.go:667] quota admission added evaluator for: namespaces
I1130 16:37:54.919299       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I1130 16:37:54.921277       1 default_servicecidr_controller.go:228] Setting default ServiceCIDR condition Ready to True
I1130 16:37:54.948005       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1130 16:37:54.949610       1 default_servicecidr_controller.go:137] Shutting down kubernetes-service-cidr-controller
I1130 16:37:55.059698       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
I1130 16:37:55.822944       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I1130 16:37:55.842257       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I1130 16:37:55.842291       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1130 16:37:56.155532       1 controller.go:667] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I1130 16:37:56.177669       1 controller.go:667] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I1130 16:37:56.203617       1 alloc.go:328] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W1130 16:37:56.207934       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I1130 16:37:56.208453       1 controller.go:667] quota admission added evaluator for: endpoints
I1130 16:37:56.210774       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1130 16:37:56.816540       1 controller.go:667] quota admission added evaluator for: serviceaccounts
I1130 16:37:57.534943       1 controller.go:667] quota admission added evaluator for: deployments.apps
I1130 16:37:57.547861       1 alloc.go:328] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I1130 16:37:57.557827       1 controller.go:667] quota admission added evaluator for: daemonsets.apps
I1130 16:38:02.419223       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1130 16:38:02.421817       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1130 16:38:02.464493       1 controller.go:667] quota admission added evaluator for: controllerrevisions.apps
I1130 16:38:02.865080       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I1130 16:38:32.112647       1 controller.go:667] quota admission added evaluator for: horizontalpodautoscalers.autoscaling
I1130 16:38:32.138292       1 alloc.go:328] "allocated clusterIPs" service="default/backend" clusterIPs={"IPv4":"10.97.235.123"}
I1130 16:38:32.156319       1 alloc.go:328] "allocated clusterIPs" service="default/mongo" clusterIPs={"IPv4":"10.103.220.187"}
I1130 16:38:32.165759       1 controller.go:667] quota admission added evaluator for: statefulsets.apps
I1130 16:38:32.196885       1 alloc.go:328] "allocated clusterIPs" service="default/nginx" clusterIPs={"IPv4":"10.96.62.92"}
I1130 16:38:32.223465       1 alloc.go:328] "allocated clusterIPs" service="default/studentportfolio" clusterIPs={"IPv4":"10.103.208.81"}
I1130 16:38:32.254899       1 alloc.go:328] "allocated clusterIPs" service="default/transactions" clusterIPs={"IPv4":"10.99.209.254"}
I1130 16:38:58.667851       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 16:39:02.051622       1 stats.go:136] "Error getting keys" err="empty key: \"\""


==> kube-controller-manager [b5be2b9a0914] <==
I1130 16:38:01.735799       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I1130 16:38:01.741165       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice"
I1130 16:38:01.749786       1 shared_informer.go:356] "Caches are synced" controller="namespace"
I1130 16:38:01.755320       1 shared_informer.go:356] "Caches are synced" controller="stateful set"
I1130 16:38:01.761932       1 shared_informer.go:356] "Caches are synced" controller="PVC protection"
I1130 16:38:01.763546       1 shared_informer.go:356] "Caches are synced" controller="HPA"
I1130 16:38:01.763590       1 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"
I1130 16:38:01.765755       1 shared_informer.go:356] "Caches are synced" controller="attach detach"
I1130 16:38:01.765806       1 shared_informer.go:356] "Caches are synced" controller="resource_claim"
I1130 16:38:01.766000       1 shared_informer.go:356] "Caches are synced" controller="job"
I1130 16:38:01.766560       1 shared_informer.go:356] "Caches are synced" controller="daemon sets"
I1130 16:38:01.766635       1 shared_informer.go:356] "Caches are synced" controller="service-cidr-controller"
I1130 16:38:01.766737       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I1130 16:38:01.765782       1 shared_informer.go:356] "Caches are synced" controller="ephemeral"
I1130 16:38:01.766976       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I1130 16:38:01.767125       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I1130 16:38:01.767148       1 shared_informer.go:356] "Caches are synced" controller="persistent volume"
I1130 16:38:01.767153       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I1130 16:38:01.768115       1 shared_informer.go:356] "Caches are synced" controller="ReplicationController"
I1130 16:38:01.768982       1 shared_informer.go:356] "Caches are synced" controller="TTL after finished"
I1130 16:38:01.769088       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice_mirroring"
I1130 16:38:01.771060       1 shared_informer.go:356] "Caches are synced" controller="disruption"
I1130 16:38:01.786439       1 shared_informer.go:356] "Caches are synced" controller="ReplicaSet"
I1130 16:38:01.812761       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrapproving"
I1130 16:38:01.812900       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I1130 16:38:01.812980       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I1130 16:38:01.812997       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I1130 16:38:01.814711       1 shared_informer.go:356] "Caches are synced" controller="expand"
I1130 16:38:01.814855       1 shared_informer.go:356] "Caches are synced" controller="GC"
I1130 16:38:01.814938       1 shared_informer.go:356] "Caches are synced" controller="cronjob"
I1130 16:38:01.815026       1 shared_informer.go:356] "Caches are synced" controller="TTL"
I1130 16:38:01.816659       1 shared_informer.go:356] "Caches are synced" controller="ClusterRoleAggregator"
I1130 16:38:01.816776       1 shared_informer.go:356] "Caches are synced" controller="PV protection"
I1130 16:38:01.817059       1 shared_informer.go:356] "Caches are synced" controller="service account"
I1130 16:38:01.817116       1 shared_informer.go:356] "Caches are synced" controller="VAC protection"
I1130 16:38:01.817197       1 shared_informer.go:356] "Caches are synced" controller="crt configmap"
I1130 16:38:01.819393       1 shared_informer.go:356] "Caches are synced" controller="legacy-service-account-token-cleaner"
I1130 16:38:01.820922       1 shared_informer.go:356] "Caches are synced" controller="taint-eviction-controller"
I1130 16:38:01.821651       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1130 16:38:01.821726       1 shared_informer.go:356] "Caches are synced" controller="bootstrap_signer"
I1130 16:38:01.825295       1 shared_informer.go:356] "Caches are synced" controller="deployment"
I1130 16:38:01.827699       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1130 16:38:01.832212       1 shared_informer.go:356] "Caches are synced" controller="node"
I1130 16:38:01.832344       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I1130 16:38:01.832351       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I1130 16:38:01.832380       1 shared_informer.go:356] "Caches are synced" controller="taint"
I1130 16:38:01.832386       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I1130 16:38:01.832649       1 shared_informer.go:349] "Waiting for caches to sync" controller="cidrallocator"
I1130 16:38:01.832674       1 shared_informer.go:356] "Caches are synced" controller="cidrallocator"
I1130 16:38:01.832558       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I1130 16:38:01.832852       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I1130 16:38:01.832932       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I1130 16:38:01.837143       1 shared_informer.go:356] "Caches are synced" controller="endpoint"
I1130 16:38:01.843295       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
E1130 16:38:46.369561       1 horizontal.go:279] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/backend: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)" logger="UnhandledError"
E1130 16:38:46.469634       1 horizontal.go:279] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/transactions: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)" logger="UnhandledError"
E1130 16:39:01.384547       1 horizontal.go:279] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/backend: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)" logger="UnhandledError"
E1130 16:39:01.477858       1 horizontal.go:279] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/transactions: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)" logger="UnhandledError"
E1130 16:39:15.611044       1 horizontal.go:279] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/backend: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)" logger="UnhandledError"
E1130 16:39:15.703403       1 horizontal.go:279] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/transactions: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)" logger="UnhandledError"


==> kube-proxy [b0be3d891d54] <==
I1130 16:38:03.605835       1 server_linux.go:53] "Using iptables proxy"
I1130 16:38:03.739753       1 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
I1130 16:38:03.840379       1 shared_informer.go:356] "Caches are synced" controller="node informer cache"
I1130 16:38:03.840416       1 server.go:219] "Successfully retrieved NodeIPs" NodeIPs=["192.168.49.2"]
E1130 16:38:03.840562       1 server.go:256] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1130 16:38:03.857112       1 server.go:265] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1130 16:38:03.857198       1 server_linux.go:132] "Using iptables Proxier"
I1130 16:38:03.860560       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1130 16:38:03.861023       1 server.go:527] "Version info" version="v1.34.0"
I1130 16:38:03.861065       1 server.go:529] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1130 16:38:03.862911       1 config.go:106] "Starting endpoint slice config controller"
I1130 16:38:03.862950       1 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint slice config"
I1130 16:38:03.862961       1 config.go:200] "Starting service config controller"
I1130 16:38:03.862972       1 shared_informer.go:349] "Waiting for caches to sync" controller="service config"
I1130 16:38:03.862980       1 config.go:403] "Starting serviceCIDR config controller"
I1130 16:38:03.862984       1 shared_informer.go:349] "Waiting for caches to sync" controller="serviceCIDR config"
I1130 16:38:03.863021       1 config.go:309] "Starting node config controller"
I1130 16:38:03.863024       1 shared_informer.go:349] "Waiting for caches to sync" controller="node config"
I1130 16:38:03.863027       1 shared_informer.go:356] "Caches are synced" controller="node config"
I1130 16:38:03.963106       1 shared_informer.go:356] "Caches are synced" controller="service config"
I1130 16:38:03.963191       1 shared_informer.go:356] "Caches are synced" controller="serviceCIDR config"
I1130 16:38:03.963201       1 shared_informer.go:356] "Caches are synced" controller="endpoint slice config"


==> kube-scheduler [5d3154e6f15c] <==
I1130 16:37:54.119845       1 serving.go:386] Generated self-signed cert in-memory
W1130 16:37:54.804823       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1130 16:37:54.804850       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1130 16:37:54.804857       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W1130 16:37:54.804863       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1130 16:37:54.824654       1 server.go:175] "Starting Kubernetes Scheduler" version="v1.34.0"
I1130 16:37:54.824680       1 server.go:177] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1130 16:37:54.825884       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1130 16:37:54.825932       1 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1130 16:37:54.826026       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I1130 16:37:54.826099       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
E1130 16:37:54.827057       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E1130 16:37:54.827299       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E1130 16:37:54.827312       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E1130 16:37:54.827628       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E1130 16:37:54.827682       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E1130 16:37:54.827685       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E1130 16:37:54.827710       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E1130 16:37:54.827673       1 reflector.go:205] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E1130 16:37:54.827716       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceSlice: resourceslices.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceslices\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice"
E1130 16:37:54.827788       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E1130 16:37:54.827716       1 reflector.go:205] "Failed to watch" err="failed to list *v1.DeviceClass: deviceclasses.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"deviceclasses\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass"
E1130 16:37:54.827830       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E1130 16:37:54.827839       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E1130 16:37:54.827864       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E1130 16:37:54.827881       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E1130 16:37:54.827887       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E1130 16:37:54.827932       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceClaim: resourceclaims.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceclaims\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceClaim"
E1130 16:37:54.827953       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E1130 16:37:54.828040       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E1130 16:37:55.676698       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E1130 16:37:55.704863       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceSlice: resourceslices.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceslices\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice"
E1130 16:37:55.740171       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E1130 16:37:55.838278       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E1130 16:37:55.896994       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E1130 16:37:55.932202       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E1130 16:37:55.937087       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E1130 16:37:55.982886       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E1130 16:37:55.999594       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E1130 16:37:56.009436       1 reflector.go:205] "Failed to watch" err="failed to list *v1.DeviceClass: deviceclasses.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"deviceclasses\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass"
I1130 16:37:58.226545       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E1130 16:38:32.186415       1 schedule_one.go:191] "Status after running PostFilter plugins for pod" logger="UnhandledError" pod="default/mongo-0" status="not found"
E1130 16:38:32.193402       1 schedule_one.go:191] "Status after running PostFilter plugins for pod" logger="UnhandledError" pod="default/mongo-0" status="not found"
E1130 16:38:32.204054       1 schedule_one.go:191] "Status after running PostFilter plugins for pod" logger="UnhandledError" pod="default/mongo-0" status="not found"
E1130 16:38:32.209764       1 schedule_one.go:191] "Status after running PostFilter plugins for pod" logger="UnhandledError" pod="default/mongo-0" status="not found"
E1130 16:38:32.213861       1 schedule_one.go:191] "Status after running PostFilter plugins for pod" logger="UnhandledError" pod="default/mongo-0" status="not found"


==> kubelet <==
Nov 30 16:38:40 minikube kubelet[2283]: I1130 16:38:40.451974    2283 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"e63c00ade388d5a0c301ba57f954764b6b649841d68d31b3e5698b8baaa6c9cf"} err="failed to get container status \"e63c00ade388d5a0c301ba57f954764b6b649841d68d31b3e5698b8baaa6c9cf\": rpc error: code = Unknown desc = Error response from daemon: No such container: e63c00ade388d5a0c301ba57f954764b6b649841d68d31b3e5698b8baaa6c9cf"
Nov 30 16:38:41 minikube kubelet[2283]: I1130 16:38:41.470663    2283 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/backend-5f6469d4f-2495h" podStartSLOduration=2.470650402 podStartE2EDuration="2.470650402s" podCreationTimestamp="2025-11-30 16:38:39 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-11-30 16:38:40.462773891 +0000 UTC m=+44.705753614" watchObservedRunningTime="2025-11-30 16:38:41.470650402 +0000 UTC m=+45.713630125"
Nov 30 16:38:41 minikube kubelet[2283]: I1130 16:38:41.850283    2283 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="8a06c4d4-9425-4b1f-84a7-a67a8438b652" path="/var/lib/kubelet/pods/8a06c4d4-9425-4b1f-84a7-a67a8438b652/volumes"
Nov 30 16:38:44 minikube kubelet[2283]: I1130 16:38:44.426361    2283 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/mongo-0" podStartSLOduration=3.693446891 podStartE2EDuration="12.426329099s" podCreationTimestamp="2025-11-30 16:38:32 +0000 UTC" firstStartedPulling="2025-11-30 16:38:32.771850692 +0000 UTC m=+36.231963787" lastFinishedPulling="2025-11-30 16:38:40.721866275 +0000 UTC m=+44.964845995" observedRunningTime="2025-11-30 16:38:41.470535896 +0000 UTC m=+45.713515618" watchObservedRunningTime="2025-11-30 16:38:44.426329099 +0000 UTC m=+48.669308856"
Nov 30 16:38:44 minikube kubelet[2283]: I1130 16:38:44.677528    2283 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-wdx9n\" (UniqueName: \"kubernetes.io/projected/0484a5b1-ed47-439a-87a8-03f3e1be33e0-kube-api-access-wdx9n\") pod \"transactions-dbdc64dd4-hh76s\" (UID: \"0484a5b1-ed47-439a-87a8-03f3e1be33e0\") " pod="default/transactions-dbdc64dd4-hh76s"
Nov 30 16:38:45 minikube kubelet[2283]: I1130 16:38:45.481746    2283 reconciler_common.go:163] "operationExecutor.UnmountVolume started for volume \"kube-api-access-cn6ld\" (UniqueName: \"kubernetes.io/projected/83fdf62b-46ce-4f51-86af-4d9838d17cfd-kube-api-access-cn6ld\") pod \"83fdf62b-46ce-4f51-86af-4d9838d17cfd\" (UID: \"83fdf62b-46ce-4f51-86af-4d9838d17cfd\") "
Nov 30 16:38:45 minikube kubelet[2283]: I1130 16:38:45.483816    2283 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/83fdf62b-46ce-4f51-86af-4d9838d17cfd-kube-api-access-cn6ld" (OuterVolumeSpecName: "kube-api-access-cn6ld") pod "83fdf62b-46ce-4f51-86af-4d9838d17cfd" (UID: "83fdf62b-46ce-4f51-86af-4d9838d17cfd"). InnerVolumeSpecName "kube-api-access-cn6ld". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Nov 30 16:38:45 minikube kubelet[2283]: I1130 16:38:45.520466    2283 scope.go:117] "RemoveContainer" containerID="487194d73e433bf967501f07b3cfcc37442b9a101ee378d843ead40d824247e4"
Nov 30 16:38:45 minikube kubelet[2283]: I1130 16:38:45.524262    2283 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/transactions-dbdc64dd4-hh76s" podStartSLOduration=1.5242520160000002 podStartE2EDuration="1.524252016s" podCreationTimestamp="2025-11-30 16:38:44 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-11-30 16:38:45.524157172 +0000 UTC m=+49.767136894" watchObservedRunningTime="2025-11-30 16:38:45.524252016 +0000 UTC m=+49.767231739"
Nov 30 16:38:45 minikube kubelet[2283]: I1130 16:38:45.532889    2283 scope.go:117] "RemoveContainer" containerID="487194d73e433bf967501f07b3cfcc37442b9a101ee378d843ead40d824247e4"
Nov 30 16:38:45 minikube kubelet[2283]: E1130 16:38:45.533557    2283 log.go:32] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 487194d73e433bf967501f07b3cfcc37442b9a101ee378d843ead40d824247e4" containerID="487194d73e433bf967501f07b3cfcc37442b9a101ee378d843ead40d824247e4"
Nov 30 16:38:45 minikube kubelet[2283]: I1130 16:38:45.533591    2283 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"487194d73e433bf967501f07b3cfcc37442b9a101ee378d843ead40d824247e4"} err="failed to get container status \"487194d73e433bf967501f07b3cfcc37442b9a101ee378d843ead40d824247e4\": rpc error: code = Unknown desc = Error response from daemon: No such container: 487194d73e433bf967501f07b3cfcc37442b9a101ee378d843ead40d824247e4"
Nov 30 16:38:45 minikube kubelet[2283]: I1130 16:38:45.582468    2283 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-cn6ld\" (UniqueName: \"kubernetes.io/projected/83fdf62b-46ce-4f51-86af-4d9838d17cfd-kube-api-access-cn6ld\") on node \"minikube\" DevicePath \"\""
Nov 30 16:38:45 minikube kubelet[2283]: I1130 16:38:45.842352    2283 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="83fdf62b-46ce-4f51-86af-4d9838d17cfd" path="/var/lib/kubelet/pods/83fdf62b-46ce-4f51-86af-4d9838d17cfd/volumes"
Nov 30 16:38:46 minikube kubelet[2283]: I1130 16:38:46.188585    2283 reconciler_common.go:163] "operationExecutor.UnmountVolume started for volume \"kube-api-access-wcgtv\" (UniqueName: \"kubernetes.io/projected/5353da73-1e84-4961-bef9-4fd645d25059-kube-api-access-wcgtv\") pod \"5353da73-1e84-4961-bef9-4fd645d25059\" (UID: \"5353da73-1e84-4961-bef9-4fd645d25059\") "
Nov 30 16:38:46 minikube kubelet[2283]: I1130 16:38:46.195286    2283 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/5353da73-1e84-4961-bef9-4fd645d25059-kube-api-access-wcgtv" (OuterVolumeSpecName: "kube-api-access-wcgtv") pod "5353da73-1e84-4961-bef9-4fd645d25059" (UID: "5353da73-1e84-4961-bef9-4fd645d25059"). InnerVolumeSpecName "kube-api-access-wcgtv". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Nov 30 16:38:46 minikube kubelet[2283]: I1130 16:38:46.290088    2283 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-wcgtv\" (UniqueName: \"kubernetes.io/projected/5353da73-1e84-4961-bef9-4fd645d25059-kube-api-access-wcgtv\") on node \"minikube\" DevicePath \"\""
Nov 30 16:38:46 minikube kubelet[2283]: I1130 16:38:46.532953    2283 scope.go:117] "RemoveContainer" containerID="9ce5fc80a327e17e2a7add0b1e442917e7e94ad7117d27e98ed165eefed4b412"
Nov 30 16:38:46 minikube kubelet[2283]: I1130 16:38:46.544108    2283 scope.go:117] "RemoveContainer" containerID="9ce5fc80a327e17e2a7add0b1e442917e7e94ad7117d27e98ed165eefed4b412"
Nov 30 16:38:46 minikube kubelet[2283]: E1130 16:38:46.544680    2283 log.go:32] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 9ce5fc80a327e17e2a7add0b1e442917e7e94ad7117d27e98ed165eefed4b412" containerID="9ce5fc80a327e17e2a7add0b1e442917e7e94ad7117d27e98ed165eefed4b412"
Nov 30 16:38:46 minikube kubelet[2283]: I1130 16:38:46.544709    2283 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"9ce5fc80a327e17e2a7add0b1e442917e7e94ad7117d27e98ed165eefed4b412"} err="failed to get container status \"9ce5fc80a327e17e2a7add0b1e442917e7e94ad7117d27e98ed165eefed4b412\": rpc error: code = Unknown desc = Error response from daemon: No such container: 9ce5fc80a327e17e2a7add0b1e442917e7e94ad7117d27e98ed165eefed4b412"
Nov 30 16:38:46 minikube kubelet[2283]: I1130 16:38:46.692633    2283 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-fw2fj\" (UniqueName: \"kubernetes.io/projected/ddee7d32-2f85-4d6f-83bc-9c2676f19747-kube-api-access-fw2fj\") pod \"backend-5f6469d4f-7hlhq\" (UID: \"ddee7d32-2f85-4d6f-83bc-9c2676f19747\") " pod="default/backend-5f6469d4f-7hlhq"
Nov 30 16:38:47 minikube kubelet[2283]: I1130 16:38:47.095749    2283 reconciler_common.go:163] "operationExecutor.UnmountVolume started for volume \"kube-api-access-jmrhr\" (UniqueName: \"kubernetes.io/projected/8b5c5b3d-ad11-4938-8c34-2d57d0d89a0d-kube-api-access-jmrhr\") pod \"8b5c5b3d-ad11-4938-8c34-2d57d0d89a0d\" (UID: \"8b5c5b3d-ad11-4938-8c34-2d57d0d89a0d\") "
Nov 30 16:38:47 minikube kubelet[2283]: I1130 16:38:47.098082    2283 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/8b5c5b3d-ad11-4938-8c34-2d57d0d89a0d-kube-api-access-jmrhr" (OuterVolumeSpecName: "kube-api-access-jmrhr") pod "8b5c5b3d-ad11-4938-8c34-2d57d0d89a0d" (UID: "8b5c5b3d-ad11-4938-8c34-2d57d0d89a0d"). InnerVolumeSpecName "kube-api-access-jmrhr". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Nov 30 16:38:47 minikube kubelet[2283]: I1130 16:38:47.196869    2283 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-jmrhr\" (UniqueName: \"kubernetes.io/projected/8b5c5b3d-ad11-4938-8c34-2d57d0d89a0d-kube-api-access-jmrhr\") on node \"minikube\" DevicePath \"\""
Nov 30 16:38:47 minikube kubelet[2283]: E1130 16:38:47.297128    2283 configmap.go:193] Couldn't get configMap default/studentportfolio-files: configmap "studentportfolio-files" not found
Nov 30 16:38:47 minikube kubelet[2283]: E1130 16:38:47.297193    2283 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/configmap/60b60a38-7d8d-4bc8-ac8c-dfebc5a8ed49-portfolio podName:60b60a38-7d8d-4bc8-ac8c-dfebc5a8ed49 nodeName:}" failed. No retries permitted until 2025-11-30 16:39:03.2971813 +0000 UTC m=+67.540161020 (durationBeforeRetry 16s). Error: MountVolume.SetUp failed for volume "portfolio" (UniqueName: "kubernetes.io/configmap/60b60a38-7d8d-4bc8-ac8c-dfebc5a8ed49-portfolio") pod "nginx-5885db8fdb-cjgn9" (UID: "60b60a38-7d8d-4bc8-ac8c-dfebc5a8ed49") : configmap "studentportfolio-files" not found
Nov 30 16:38:47 minikube kubelet[2283]: E1130 16:38:47.297205    2283 configmap.go:193] Couldn't get configMap default/studentportfolio-files: configmap "studentportfolio-files" not found
Nov 30 16:38:47 minikube kubelet[2283]: E1130 16:38:47.297265    2283 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/configmap/35f3dd5d-c479-4556-8551-9f3cbe19c993-portfolio podName:35f3dd5d-c479-4556-8551-9f3cbe19c993 nodeName:}" failed. No retries permitted until 2025-11-30 16:39:03.297253095 +0000 UTC m=+67.540232821 (durationBeforeRetry 16s). Error: MountVolume.SetUp failed for volume "portfolio" (UniqueName: "kubernetes.io/configmap/35f3dd5d-c479-4556-8551-9f3cbe19c993-portfolio") pod "nginx-5885db8fdb-6zcsk" (UID: "35f3dd5d-c479-4556-8551-9f3cbe19c993") : configmap "studentportfolio-files" not found
Nov 30 16:38:47 minikube kubelet[2283]: I1130 16:38:47.541191    2283 scope.go:117] "RemoveContainer" containerID="fee293e20ec44f7be600fa5e5cd77ea632f4e5e36178e20aaa8e3d7d6bf4ba75"
Nov 30 16:38:47 minikube kubelet[2283]: I1130 16:38:47.556456    2283 scope.go:117] "RemoveContainer" containerID="fee293e20ec44f7be600fa5e5cd77ea632f4e5e36178e20aaa8e3d7d6bf4ba75"
Nov 30 16:38:47 minikube kubelet[2283]: E1130 16:38:47.557327    2283 log.go:32] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: fee293e20ec44f7be600fa5e5cd77ea632f4e5e36178e20aaa8e3d7d6bf4ba75" containerID="fee293e20ec44f7be600fa5e5cd77ea632f4e5e36178e20aaa8e3d7d6bf4ba75"
Nov 30 16:38:47 minikube kubelet[2283]: I1130 16:38:47.557359    2283 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"fee293e20ec44f7be600fa5e5cd77ea632f4e5e36178e20aaa8e3d7d6bf4ba75"} err="failed to get container status \"fee293e20ec44f7be600fa5e5cd77ea632f4e5e36178e20aaa8e3d7d6bf4ba75\": rpc error: code = Unknown desc = Error response from daemon: No such container: fee293e20ec44f7be600fa5e5cd77ea632f4e5e36178e20aaa8e3d7d6bf4ba75"
Nov 30 16:38:47 minikube kubelet[2283]: I1130 16:38:47.564390    2283 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/backend-5f6469d4f-7hlhq" podStartSLOduration=1.5643778830000001 podStartE2EDuration="1.564377883s" podCreationTimestamp="2025-11-30 16:38:46 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-11-30 16:38:47.556637986 +0000 UTC m=+51.799617708" watchObservedRunningTime="2025-11-30 16:38:47.564377883 +0000 UTC m=+51.807357605"
Nov 30 16:38:47 minikube kubelet[2283]: I1130 16:38:47.839918    2283 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="5353da73-1e84-4961-bef9-4fd645d25059" path="/var/lib/kubelet/pods/5353da73-1e84-4961-bef9-4fd645d25059/volumes"
Nov 30 16:38:47 minikube kubelet[2283]: I1130 16:38:47.840761    2283 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="8b5c5b3d-ad11-4938-8c34-2d57d0d89a0d" path="/var/lib/kubelet/pods/8b5c5b3d-ad11-4938-8c34-2d57d0d89a0d/volumes"
Nov 30 16:38:48 minikube kubelet[2283]: E1130 16:38:48.305452    2283 configmap.go:193] Couldn't get configMap default/studentportfolio-files: configmap "studentportfolio-files" not found
Nov 30 16:38:48 minikube kubelet[2283]: E1130 16:38:48.305540    2283 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/configmap/894cad43-fa0e-4c13-b087-520612510c53-portfolio podName:894cad43-fa0e-4c13-b087-520612510c53 nodeName:}" failed. No retries permitted until 2025-11-30 16:39:04.305529228 +0000 UTC m=+68.548508953 (durationBeforeRetry 16s). Error: MountVolume.SetUp failed for volume "portfolio" (UniqueName: "kubernetes.io/configmap/894cad43-fa0e-4c13-b087-520612510c53-portfolio") pod "nginx-6cf6b94dd4-5sw7n" (UID: "894cad43-fa0e-4c13-b087-520612510c53") : configmap "studentportfolio-files" not found
Nov 30 16:38:54 minikube kubelet[2283]: I1130 16:38:54.053154    2283 reconciler_common.go:163] "operationExecutor.UnmountVolume started for volume \"kube-api-access-gfnl2\" (UniqueName: \"kubernetes.io/projected/a5275c5d-c683-4164-adea-7649c9e692ba-kube-api-access-gfnl2\") pod \"a5275c5d-c683-4164-adea-7649c9e692ba\" (UID: \"a5275c5d-c683-4164-adea-7649c9e692ba\") "
Nov 30 16:38:54 minikube kubelet[2283]: I1130 16:38:54.056921    2283 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/a5275c5d-c683-4164-adea-7649c9e692ba-kube-api-access-gfnl2" (OuterVolumeSpecName: "kube-api-access-gfnl2") pod "a5275c5d-c683-4164-adea-7649c9e692ba" (UID: "a5275c5d-c683-4164-adea-7649c9e692ba"). InnerVolumeSpecName "kube-api-access-gfnl2". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Nov 30 16:38:54 minikube kubelet[2283]: I1130 16:38:54.154392    2283 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-gfnl2\" (UniqueName: \"kubernetes.io/projected/a5275c5d-c683-4164-adea-7649c9e692ba-kube-api-access-gfnl2\") on node \"minikube\" DevicePath \"\""
Nov 30 16:38:54 minikube kubelet[2283]: I1130 16:38:54.680687    2283 scope.go:117] "RemoveContainer" containerID="b0a6460701751d1b458aa9795b7428f49377840c6255a6c051045b45515085ea"
Nov 30 16:38:54 minikube kubelet[2283]: I1130 16:38:54.688637    2283 scope.go:117] "RemoveContainer" containerID="b0a6460701751d1b458aa9795b7428f49377840c6255a6c051045b45515085ea"
Nov 30 16:38:54 minikube kubelet[2283]: E1130 16:38:54.689311    2283 log.go:32] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: b0a6460701751d1b458aa9795b7428f49377840c6255a6c051045b45515085ea" containerID="b0a6460701751d1b458aa9795b7428f49377840c6255a6c051045b45515085ea"
Nov 30 16:38:54 minikube kubelet[2283]: I1130 16:38:54.689340    2283 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"b0a6460701751d1b458aa9795b7428f49377840c6255a6c051045b45515085ea"} err="failed to get container status \"b0a6460701751d1b458aa9795b7428f49377840c6255a6c051045b45515085ea\": rpc error: code = Unknown desc = Error response from daemon: No such container: b0a6460701751d1b458aa9795b7428f49377840c6255a6c051045b45515085ea"
Nov 30 16:38:55 minikube kubelet[2283]: I1130 16:38:55.837858    2283 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="a5275c5d-c683-4164-adea-7649c9e692ba" path="/var/lib/kubelet/pods/a5275c5d-c683-4164-adea-7649c9e692ba/volumes"
Nov 30 16:38:57 minikube kubelet[2283]: I1130 16:38:57.585553    2283 reconciler_common.go:163] "operationExecutor.UnmountVolume started for volume \"kube-api-access-djs5r\" (UniqueName: \"kubernetes.io/projected/3e91bb8e-a498-4967-837a-a4736b7866ec-kube-api-access-djs5r\") pod \"3e91bb8e-a498-4967-837a-a4736b7866ec\" (UID: \"3e91bb8e-a498-4967-837a-a4736b7866ec\") "
Nov 30 16:38:57 minikube kubelet[2283]: I1130 16:38:57.588867    2283 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/3e91bb8e-a498-4967-837a-a4736b7866ec-kube-api-access-djs5r" (OuterVolumeSpecName: "kube-api-access-djs5r") pod "3e91bb8e-a498-4967-837a-a4736b7866ec" (UID: "3e91bb8e-a498-4967-837a-a4736b7866ec"). InnerVolumeSpecName "kube-api-access-djs5r". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Nov 30 16:38:57 minikube kubelet[2283]: I1130 16:38:57.687041    2283 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-djs5r\" (UniqueName: \"kubernetes.io/projected/3e91bb8e-a498-4967-837a-a4736b7866ec-kube-api-access-djs5r\") on node \"minikube\" DevicePath \"\""
Nov 30 16:38:57 minikube kubelet[2283]: I1130 16:38:57.734301    2283 scope.go:117] "RemoveContainer" containerID="68350536d2a47202319dbff09351af8f1b0d8da1937c3fa0ec141c7923bd014b"
Nov 30 16:38:57 minikube kubelet[2283]: I1130 16:38:57.749838    2283 scope.go:117] "RemoveContainer" containerID="68350536d2a47202319dbff09351af8f1b0d8da1937c3fa0ec141c7923bd014b"
Nov 30 16:38:57 minikube kubelet[2283]: E1130 16:38:57.750438    2283 log.go:32] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 68350536d2a47202319dbff09351af8f1b0d8da1937c3fa0ec141c7923bd014b" containerID="68350536d2a47202319dbff09351af8f1b0d8da1937c3fa0ec141c7923bd014b"
Nov 30 16:38:57 minikube kubelet[2283]: I1130 16:38:57.750466    2283 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"68350536d2a47202319dbff09351af8f1b0d8da1937c3fa0ec141c7923bd014b"} err="failed to get container status \"68350536d2a47202319dbff09351af8f1b0d8da1937c3fa0ec141c7923bd014b\": rpc error: code = Unknown desc = Error response from daemon: No such container: 68350536d2a47202319dbff09351af8f1b0d8da1937c3fa0ec141c7923bd014b"
Nov 30 16:38:57 minikube kubelet[2283]: I1130 16:38:57.842457    2283 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="3e91bb8e-a498-4967-837a-a4736b7866ec" path="/var/lib/kubelet/pods/3e91bb8e-a498-4967-837a-a4736b7866ec/volumes"
Nov 30 16:39:03 minikube kubelet[2283]: E1130 16:39:03.340321    2283 configmap.go:193] Couldn't get configMap default/studentportfolio-files: configmap "studentportfolio-files" not found
Nov 30 16:39:03 minikube kubelet[2283]: E1130 16:39:03.340401    2283 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/configmap/60b60a38-7d8d-4bc8-ac8c-dfebc5a8ed49-portfolio podName:60b60a38-7d8d-4bc8-ac8c-dfebc5a8ed49 nodeName:}" failed. No retries permitted until 2025-11-30 16:39:35.340391078 +0000 UTC m=+99.583370800 (durationBeforeRetry 32s). Error: MountVolume.SetUp failed for volume "portfolio" (UniqueName: "kubernetes.io/configmap/60b60a38-7d8d-4bc8-ac8c-dfebc5a8ed49-portfolio") pod "nginx-5885db8fdb-cjgn9" (UID: "60b60a38-7d8d-4bc8-ac8c-dfebc5a8ed49") : configmap "studentportfolio-files" not found
Nov 30 16:39:03 minikube kubelet[2283]: E1130 16:39:03.340334    2283 configmap.go:193] Couldn't get configMap default/studentportfolio-files: configmap "studentportfolio-files" not found
Nov 30 16:39:03 minikube kubelet[2283]: E1130 16:39:03.340420    2283 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/configmap/35f3dd5d-c479-4556-8551-9f3cbe19c993-portfolio podName:35f3dd5d-c479-4556-8551-9f3cbe19c993 nodeName:}" failed. No retries permitted until 2025-11-30 16:39:35.340417227 +0000 UTC m=+99.583396941 (durationBeforeRetry 32s). Error: MountVolume.SetUp failed for volume "portfolio" (UniqueName: "kubernetes.io/configmap/35f3dd5d-c479-4556-8551-9f3cbe19c993-portfolio") pod "nginx-5885db8fdb-6zcsk" (UID: "35f3dd5d-c479-4556-8551-9f3cbe19c993") : configmap "studentportfolio-files" not found
Nov 30 16:39:03 minikube kubelet[2283]: E1130 16:39:03.571880    2283 configmap.go:193] Couldn't get configMap default/studentportfolio-files: configmap "studentportfolio-files" not found
Nov 30 16:39:03 minikube kubelet[2283]: E1130 16:39:03.571958    2283 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/configmap/894cad43-fa0e-4c13-b087-520612510c53-portfolio podName:894cad43-fa0e-4c13-b087-520612510c53 nodeName:}" failed. No retries permitted until 2025-11-30 16:39:35.571946842 +0000 UTC m=+100.592924215 (durationBeforeRetry 32s). Error: MountVolume.SetUp failed for volume "portfolio" (UniqueName: "kubernetes.io/configmap/894cad43-fa0e-4c13-b087-520612510c53-portfolio") pod "nginx-6cf6b94dd4-5sw7n" (UID: "894cad43-fa0e-4c13-b087-520612510c53") : configmap "studentportfolio-files" not found


==> storage-provisioner [cb3350eb53eb] <==
I1130 16:38:32.193924       1 controller.go:1456] provision "default/mongo-data-mongo-0" class "standard": succeeded
I1130 16:38:32.193927       1 volume_store.go:212] Trying to save persistentvolume "pvc-7c6adf91-2fca-4612-9828-8f90cf4109a7"
I1130 16:38:32.204114       1 volume_store.go:219] persistentvolume "pvc-7c6adf91-2fca-4612-9828-8f90cf4109a7" saved
I1130 16:38:32.204347       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"mongo-data-mongo-0", UID:"7c6adf91-2fca-4612-9828-8f90cf4109a7", APIVersion:"v1", ResourceVersion:"493", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-7c6adf91-2fca-4612-9828-8f90cf4109a7
W1130 16:38:33.539435       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:38:33.542907       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:38:34.762329       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:38:34.764659       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:38:36.767482       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:38:36.770030       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:38:38.772838       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:38:38.775401       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:38:40.778854       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:38:40.781444       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:38:42.786740       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:38:42.802416       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:38:44.805944       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:38:44.836920       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:38:46.839349       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:38:46.842216       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:38:48.851056       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:38:48.863438       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:38:50.876071       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:38:50.895542       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:38:52.899403       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:38:52.920716       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:38:54.925125       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:38:54.928165       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:38:56.933820       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:38:56.938445       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:38:58.942182       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:38:58.964415       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:39:00.968759       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:39:00.989073       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:39:02.993308       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:39:02.997212       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:39:04.222849       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:39:04.260814       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:39:06.263303       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:39:06.282204       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:39:08.284692       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:39:08.297602       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:39:10.300332       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:39:10.322360       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:39:12.324839       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:39:12.347496       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:39:14.350048       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:39:14.394591       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:39:16.396761       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:39:16.412790       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:39:18.415618       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:39:18.436220       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:39:20.438449       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:39:20.461670       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:39:22.464396       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:39:22.467166       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:39:24.469396       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:39:24.488528       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:39:26.490913       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 16:39:26.508854       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice

